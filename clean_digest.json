[
  {
    "title": "LEANN – Local RAG with 97% smaller index and Claude Code–compatible semantic search",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1ml846a/leann_local_rag_with_97_smaller_index_and_claude/",
    "content": "We’re building LEANN at Berkeley Sky Lab — a local vector index for RAG that’s:  🔒 Privacy-first 📦 97% smaller 🧠 Fully compatible with Claude Code, Ollama, and GPT-OSS  Run semantic search on your laptop — fast, lightweight, and cloud-free. 🧠 Why does LEANN matter? Most vector databases store everything — every embedding, every edge — which quickly balloons to 100+ GB when indexing emails, chat, and code. (For example, embedding just 50 GB of text can require over 500 GB of storage.) But most queries only touch a tiny slice of the DB. So we asked: Why store every single embedding? ⚙️ LEANN introduces two ultra-lightweight backends:  🔍 Graph-only mode Stores no embeddings, just a pruned HNSW graph. Recomputes embeddings on-the-fly using overlapping neighbors. 💡 PQ+Rerank mode compresses vectors with PQ and replaces heavy storage with lightweight recomputation over the candidate set.  Each has different tradeoffs, but both achieve the same goal:  🧠 Massive storage savings with no meaningful drop in recall  📝 Note: In modern RAG systems — with long contexts and reasoning-heavy models — generation, not retrieval, is the bottleneck. So even with slightly slower retrieval, total latency increases by just ~5% or less. 🔍 LEANN supports semantic search over:  📨 Apple Mail 💾 Filesystem 🕰️ Chrome / Chat history 🧠 Codebase (Claude Code–compatible)   LEANN = your personal Jarvis, running locally.  🔗 Links  GitHub: https://github.com/yichuan-w/LEANN Paper: https://arxiv.org/abs/2506.08276  We’d love for you to try it out, give feedback, or ask questions in the repo! 🙌  &#32; submitted by &#32;  /u/Lanky-District9096   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • Lanky-District9096 LEANN – Local RAG with 97% smaller index and Claude Code–compatible semantic search Resources We’re building LEANN at Berkeley Sky Lab — a local vector index for RAG that’s: 🔒 Privacy-first 📦 97% smaller 🧠 Fully compatible with Claude Code , Ollama , and GPT-OSS Run semantic search on your laptop — fast, lightweight, and cloud-free. 🧠 Why does LEANN matter? Most vector databases store everything — every embedding, every edge — which quickly balloons to 100+ GB when indexing emails, chat, and code. (For example, embedding just 50 GB of text can require over 500 GB of storage.) But most queries only touch a tiny slice of the DB. So we asked: Why store every single embedding? ⚙️ LEANN introduces two ultra-lightweight backends: 🔍 Graph-only mode Stores no embeddings , just a pruned HNSW graph. Recomputes embeddings on-the-fly using overlapping neighbors. 💡 PQ+Rerank mode compresses vectors with PQ and replaces heavy storage with lightweight recomputation over the candidate set. Each has different tradeoffs, but both achieve the same goal: 🧠 Massive storage savings with no meaningful drop in recall 📝 Note: In modern RAG systems — with long contexts and reasoning-heavy models — generation , not retrieval, is the bottleneck. So even with slightly slower retrieval, total latency increases by just ~5% or less. 🔍 LEANN supports semantic search over: 📨 Apple Mail 💾 Filesystem 🕰️ Chrome / Chat history 🧠 Codebase (Claude Code–compatible) LEANN = your personal Jarvis , running locally. 🔗 Links GitHub: https://github.com/yichuan-w/LEANN Paper: https://arxiv.org/abs/2506.08276 We’d love for you to try it out, give feedback, or ask questions in the repo! 🙌 Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-08 21:48",
    "_score": 12
  },
  {
    "title": "[R] Adaptive Classifiers: Few-Shot Learning with Continuous Adaptation and Dynamic Class Addition",
    "link": "https://www.reddit.com/r/MachineLearning/comments/1mldqbb/r_adaptive_classifiers_fewshot_learning_with/",
    "content": "Paper/Blog: https://huggingface.co/blog/codelion/adaptive-classifier Code: https://github.com/codelion/adaptive-classifier Models: https://huggingface.co/adaptive-classifier TL;DR We developed an architecture that enables text classifiers to:  Learn from as few as 5-10 examples per class (few-shot) Continuously adapt to new examples without catastrophic forgetting Dynamically add new classes without retraining Achieve 90-100% accuracy on enterprise tasks with minimal data  Technical Contribution The Problem: Traditional fine-tuning requires extensive labeled data and full retraining for new classes. Current few-shot approaches don't support continuous learning or dynamic class addition. Our Solution: Combines prototype learning with elastic weight consolidation in a unified architecture: ModernBERT Encoder → Adaptive Neural Head → Prototype Memory (FAISS) ↓ EWC Regularization  Key Components:  Prototype Memory: FAISS-backed storage of learned class representations Adaptive Neural Head: Trainable layer that grows with new classes EWC Protection: Prevents forgetting when learning new examples Dynamic Architecture: Seamlessly handles new classes without architectural changes  Experimental Results Evaluated on 17 diverse text classification tasks with only 100 examples per class: Standout Results:  Fraud Detection: 100% accuracy Document Classification: 97.5% accuracy Support Ticket Routing: 96.8% accuracy Average across all tasks: 93.2% accuracy  Few-Shot Performance:  5 examples/class: ~85% accuracy 10 examples/class: ~90% accuracy 100 examples/class: ~93% accuracy  Continuous Learning: No accuracy degradation after learning 10+ new classes sequentially (vs 15-20% drop with naive fine-tuning). Novel Aspects  True Few-Shot Learning: Unlike prompt-based methods, learns actual task-specific representations Catastrophic Forgetting Resistance: EWC ensures old knowledge is preserved Dynamic Class Addition: Architecture grows seamlessly - no predefined class limits Memory Efficiency: Constant memory footprint regardless of training data size Fast Inference: 90-120ms (comparable to fine-tuned BERT, faster than LLM APIs)  Comparison with Existing Approaches   Method Training Examples New Classes Forgetting Inference Speed    Fine-tuned BERT 1000+ Retrain all High Fast   Prompt Engineering 0-5 Dynamic None Slow (API)   Meta-Learning 100+ Limited Medium Fast   Ours 5-100 Dynamic Minimal Fast   Implementation Details Based on ModernBERT for computational efficiency. The prototype memory uses cosine similarity for class prediction, while EWC selectively protects important weights during updates. Training Objective: L = L_classification + λ_ewc * L_ewc + λ_prototype * L_prototype  Where L_ewc prevents forgetting and L_prototype maintains class separation in embedding space. Broader Impact This work addresses a critical gap in practical ML deployment where labeled data is scarce but requirements evolve rapidly. The approach is particularly relevant for:  Domain adaptation scenarios Real-time learning systems Resource-constrained environments Evolving classification taxonomies  Future Work  Multi-modal extensions (text + vision) Theoretical analysis of forgetting bounds Scaling to 1000+ classes Integration with foundation model architectures  The complete technical details, experimental setup, and ablation studies are available in our blog post. We've also released 17 pre-trained models covering common enterprise use cases. Questions welcome! Happy to discuss the technical details, experimental choices, or potential extensions.  &#32; submitted by &#32;  /u/asankhs   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nPaper/Blog: https://huggingface.co/blog/codelion/adaptive-classifier\n\nCode: https://github.com/codelion/adaptive-classifier\n\nModels: https://huggingface.co/adaptive-classifier\n\nTL;DR\n\nWe developed an architecture that enables text classifiers to:\n\nLearn from as few as 5-10 examples per class (few-shot)\n\nContinuously adapt to new examples without catastrophic forgetting\n\nDynamically add new classes without retraining\n\nAchieve 90-100% accuracy on enterprise tasks with minimal data\n\nTechnical Contribution\n\nThe Problem: Traditional fine-tuning requires extensive labeled data and full retraining for new classes. Current few-shot approaches don't support continuous learning or dynamic class addition.\n\nOur Solution: Combines prototype learning with elastic weight consolidation in a unified architecture:\n\nModernBERT Encoder → Adaptive Neural Head → Prototype Memory (FAISS) ↓ EWC Regularization\n\nKey Components:\n\nPrototype Memory: FAISS-backed storage of learned class representations Adaptive Neural Head: Trainable layer that grows with new classes EWC Protection: Prevents forgetting when learning new examples Dynamic Architecture: Seamlessly handles new classes without architectural changes\n\nExperimental Results\n\nEvaluated on 17 diverse text classification tasks with only 100 examples per class:\n\nStandout Results:\n\nFraud Detection: 100% accuracy\n\nDocument Classification: 97.5% accuracy\n\nSupport Ticket Routing: 96.8% accuracy\n\nAverage across all tasks: 93.2% accuracy\n\nFew-Shot Performance:\n\n5 examples/class: ~85% accuracy\n\n10 examples/class: ~90% accuracy\n\n100 examples/class: ~93% accuracy\n\nContinuous Learning: No accuracy degradation after learning 10+ new classes sequentially (vs 15-20% drop with naive fine-tuning).\n\nNovel Aspects\n\nTrue Few-Shot Learning: Unlike prompt-based methods, learns actual task-specific representations Catastrophic Forgetting Resistance: EWC ensures old knowledge is preserved Dynamic Class Addition: Architecture grows seamlessly - no predefined class limits Memory Efficiency: Constant memory footprint regardless of training data size Fast Inference: 90-120ms (comparable to fine-tuned BERT, faster than LLM APIs)\n\nComparison with Existing Approaches\n\nMethod Training Examples New Classes Forgetting Inference Speed Fine-tuned BERT 1000+ Retrain all High Fast Prompt Engineering 0-5 Dynamic None Slow (API) Meta-Learning 100+ Limited Medium Fast Ours 5-100 Dynamic Minimal Fast\n\nImplementation Details\n\nBased on ModernBERT for computational efficiency. The prototype memory uses cosine similarity for class prediction, while EWC selectively protects important weights during updates.\n\nTraining Objective:\n\nL = L_classification + λ_ewc * L_ewc + λ_prototype * L_prototype\n\nWhere L_ewc prevents forgetting and L_prototype maintains class separation in embedding space.\n\nBroader Impact\n\nThis work addresses a critical gap in practical ML deployment where labeled data is scarce but requirements evolve rapidly. The approach is particularly relevant for:\n\nDomain adaptation scenarios\n\nReal-time learning systems\n\nResource-constrained environments\n\nEvolving classification taxonomies\n\nFuture Work\n\nMulti-modal extensions (text + vision)\n\nTheoretical analysis of forgetting bounds\n\nScaling to 1000+ classes\n\nIntegration with foundation model architectures\n\nThe complete technical details, experimental setup, and ablation studies are available in our blog post. We've also released 17 pre-trained models covering common enterprise use cases.\n\nQuestions welcome! Happy to discuss the technical details, experimental choices, or potential extensions.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 02:05",
    "_score": 11
  },
  {
    "title": "My thoughts on gpt-oss-120b",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlomlb/my_thoughts_on_gptoss120b/",
    "content": "Since the model dropped, it's become notoriously hated on for its censorship. (Idk what people were expecting from OpenAI of all companies) All the chat template issues and performance fluctuations with varying cloud providers made it even worse for all the people who were optimistic to try it out.  On the first day, I remember the model rejecting my request to generate some code with: I'm sorry, I cannot provide long lines of code as per my policy (or something ridiculous like this) A lot of the decisions were new: Harmony chat template, MXFP4 format for full precision and confusing quantization options. The model's inference speed was also unusually slow for 5.1B active params (I got 10-15tps initially). And naturally, I decided to wait a little for things to settle down a bit before getting to test the model. I just downloaded the original HF repo from openai and waited a few days. Yesterday, I pulled the latest chat template changes from HF repo, pulled the latest llama.cpp code, changed the model template file to ensure I set the model's default reasoning to &quot;high&quot;(since my day-to-day tasks mostly deal with reasoning, coding and instruction following), made a local MXFP4 quant myself, set the right inference settings from Unsloth docs and took it for a spin. Right off the bat, I noticed a lot of improvements! My inference speed went up to 35tps(!), the weird rejections were gone and the model felt actually usable. Ran a few of my smoke tests to see what the vibe is like and I feel it's quite good for its size. Sharing a few below:  Prompt: Generate a website for a SaaS startup titled Steve's PC Repair. The startup is a cloud rental service offering refurbished multi-gpu systems for users to rent compute on. The site needs to fit the modern tech aesthetic and be a step above current competitors offerings. Return the code in a single self-contained code file. Result: https://codepen.io/Loki-the-animator/pen/RNWZQbv  Prompt: Create me a visualization using javascript. Make it as interesting as you possibly can... Result: https://codepen.io/Loki-the-animator/pen/empEVmW  Prompt: Which word comes next: Stone, Often, Canine, _: A Helpful B Freight C Glow D Grape Answer: B – Freight Why? Each word hides the name of a number inside it:  Stone → “ONE” (st ONE) = 1 Often → “TEN” (of TEN) = 10 Canine → “NINE” (ca NINE) = 9  The next word should therefore contain the next number in the descending sequence … 8. Freight contains “EIGHT” (fr EIGHT) and fits the pattern, while the other choices do not hide a number. So the word that comes next is Freight.  One recurring theme with the model is that it simply does only what it's asked to but it does it right. However, when you decide to invest time in your prompts, it has incredible attention to detail breaking down and adhering to the intricacies of a complex set of instructions. For example, it nailed the following prompt first try: Using the Pygame library in Python, create a simple turn-based tactical game on an 8x8 grid. Requirements:  Game Board: Create an 8x8 grid. Display it graphically. Units:  Create a Unit class. Each unit has attributes: hp (health points), attack_power, move_range (e.g., 3 tiles), and team ('blue' or 'red'). Place two &quot;blue&quot; units and two &quot;red&quot; units on the board at starting positions.  Game Flow (Turn-Based):  The game should alternate turns between the 'blue' team and the 'red' team. During a team's turn, the player can select one of their units by clicking on it.  Player Actions:  Selection: When a player clicks on one of their units during their turn, that unit becomes the &quot;selected unit.&quot; Movement: After selecting a unit, the game should highlight all valid tiles the unit can move to (any tile within its move_range, not occupied by another unit). Clicking a highlighted tile moves the unit there and ends its action for the turn. Attack: If an enemy unit is adjacent to the selected unit, clicking on the enemy unit should perform an attack. The enemy's hp is reduced by the attacker's attack_power. This ends the unit's action. A unit can either move OR attack in a turn, not both.  End Condition: The game ends when all units of one team have been defeated (HP &lt;= 0). Display a &quot;Blue Team Wins!&quot; or &quot;Red Team Wins!&quot; message.  Task: Provide the full, single-script, runnable Pygame code. The code should be well-structured. Include comments explaining the main parts of the game loop, the event handling, and the logic for movement and combat.  Additionally, to test its instruction following capabilities, I used prompt templates from: https://www.jointakeoff.com/prompts and asked it to build an e-commerce website for AI gear and this is honestly where I was blown away. It came up with a pretty comprehensive 40-step plan to build the website iteratively while fully adhering to my instructions (I could share it here but it's too long) To spice things up a little, I gave the same planner prompt to Gemini 2.5 Pro and GLM 4.5 Air Q4_0 and had a new context window pulled up with Gemini 2.5 Pro to judge all 3 results and provide a score on a scale of 1-100 based on the provided plan's feasibility and adherence to instructions:  gpt-oss-120b (high): 95 Gemini 2.5 Pro: 99 GLM 4.5 Air: 45   I ran tons and tons of such tests that I can share but they would honestly clutter the intended takeaway of this post at this point. To summarize, here are my honest impressions about the model so far: 1) The model is so far the best I've gotten to run locally in terms of instruction following. 2) Reasoning abilities are top-notch. It's minimal yet thorough and effective. I refrained from using the Qwen thinking models since they think quite extensively (though they provide good results) and I couldn't fit them into my workflow. GLM 4.5 Air thinks less but the results are not as effective as the Qwen ones. gpt-oss-120b seems like the right sweet spot for me. 3) Good coder but nothing to be blown away from. Writes error-free code and does what you ask it to. If you write comprehensive prompts, you can expect good results. 4) Have tested basic agentic capabilities and have had no issues on that front so far. Yet to do extensive tests 5) The best size-to-speed model so far. The fact that I can actually run a full-precision 120b at 30-35TPS with my setup is impressive! It's the best &lt;120B model in my books for my use cases and it's gonna be my new daily driver from here on out.  I honestly feel like its censorship and initial setup-related hiccups has led to preconceived bad opinions but you have to try it out to really understand what I'm talking about. I'm probably gonna get down-voted for this amidst all the hate but I don't really care. I'm just keepin' it real and it's a solid model!  &#32; submitted by &#32;  /u/Lowkey_LokiSN   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • Lowkey_LokiSN My thoughts on gpt-oss-120b Discussion Since the model dropped, it's become notoriously hated on for its censorship. (Idk what people were expecting from OpenAI of all companies) All the chat template issues and performance fluctuations with varying cloud providers made it even worse for all the people who were optimistic to try it out. On the first day, I remember the model rejecting my request to generate some code with: I'm sorry, I cannot provide long lines of code as per my policy (or something ridiculous like this) A lot of the decisions were new: Harmony chat template, MXFP4 format for full precision and confusing quantization options. The model's inference speed was also unusually slow for 5.1B active params (I got 10-15tps initially). And naturally, I decided to wait a little for things to settle down a bit before getting to test the model. I just downloaded the original HF repo from openai and waited a few days. Yesterday, I pulled the latest chat template changes from HF repo, pulled the latest llama.cpp code, changed the model template file to ensure I set the model's default reasoning to \"high\"(since my day-to-day tasks mostly deal with reasoning, coding and instruction following), made a local MXFP4 quant myself, set the right inference settings from Unsloth docs and took it for a spin. Right off the bat, I noticed a lot of improvements! My inference speed went up to 35tps(!), the weird rejections were gone and the model felt actually usable. Ran a few of my smoke tests to see what the vibe is like and I feel it's quite good for its size. Sharing a few below: Prompt: Generate a website for a SaaS startup titled Steve's PC Repair. The startup is a cloud rental service offering refurbished multi-gpu systems for users to rent compute on. The site needs to fit the modern tech aesthetic and be a step above current competitors offerings. Return the code in a single self-contained code file. Result: https://codepen.io/Loki-the-animator/pen/RNWZQbv Prompt: Create me a visualization using javascript. Make it as interesting as you possibly can... Result: https://codepen.io/Loki-the-animator/pen/empEVmW Prompt: Which word comes next: Stone, Often, Canine, _: A Helpful B Freight C Glow D Grape Answer: B – Freight Why? Each word hides the name of a number inside it: Stone → “ONE” (st ONE ) = 1 Often → “TEN” (of TEN ) = 10 Canine → “NINE” (ca NINE ) = 9 The next word should therefore contain the next number in the descending sequence … 8. Freight contains “EIGHT” (fr EIGHT ) and fits the pattern, while the other choices do not hide a number. So the word that comes next is Freight . One recurring theme with the model is that it simply does only what it's asked to but it does it right. However, when you decide to invest time in your prompts, it has incredible attention to",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 12:48",
    "_score": 11
  },
  {
    "title": "Run GPT-OSS with MLX or GGUF in your CLI using 1 line of code",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlwaj7/run_gptoss_with_mlx_or_gguf_in_your_cli_using_1/",
    "content": "      https://reddit.com/link/1mlwaj7/video/uxn1bma711if1/player Hi LocalLLaMA, I'm from nexa-sdk project. We've made it simple to run gpt-oss in MLX &amp; GGUF formats directly from your CLI - using one line of code.  We built this to make it easy to experiment format + backend combos for our agentic RAG application. For MacOS, we believe the gpt-oss in MLX format with 4 bit quant is an excellent choice. Models we found great in MLX &amp; GGUF  MLX: [InferenceIllusionist/gpt-oss-20b-MLX-4bit]() (thanks [InferenceIllusionist]() 🙌) GGUF: [unsloth/gpt-oss-20b-GGUF]() (thanks [unsloth]() 🙌)  The qualities are roughly the same for our use case, but performance: On an M4 Max, MLX hit ~103 tok/s, about 25% faster than GGUF.  Quick tip You can paste any Hugging Face repo name into the CLI and pull it directly: nexa pull unsloth/gpt-oss-20b-GGUF  Then just select the quant option you want directly in your CLI. We think this is easier for us to test with different quants without switching screens. Let me know your thoughts &amp; feedback! Link Github: https://github.com/NexaAI/nexa-sdk Our goal is to make local model runs frictionless across more backends. Any requests and feedback are welcome!  &#32; submitted by &#32;  /u/AlanzhuLy   [link] &#32; [comments] ",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • AlanzhuLy Run GPT-OSS with MLX or GGUF in your CLI using 1 line of code Resources Hi LocalLLaMA, I'm from nexa-sdk project. We've made it simple to run gpt-oss in MLX & GGUF formats directly from your CLI - using one line of code. We built this to make it easy to experiment format + backend combos for our agentic RAG application. For MacOS, we believe the gpt-oss in MLX format with 4 bit quant is an excellent choice. Models we found great in MLX & GGUF MLX: InferenceIllusionist/gpt-oss-20b-MLX-4bit (thanks InferenceIllusionist 🙌) GGUF: unsloth/gpt-oss-20b-GGUF (thanks unsloth 🙌) The qualities are roughly the same for our use case, but performance: On an M4 Max, MLX hit ~103 tok/s, about 25% faster than GGUF. Quick tip You can paste any Hugging Face repo name into the CLI and pull it directly: nexa pull unsloth/gpt-oss-20b-GGUF Then just select the quant option you want directly in your CLI. We think this is easier for us to test with different quants without switching screens. Let me know your thoughts & feedback! Link Github: https://github.com/NexaAI/nexa-sdk Our goal is to make local model runs frictionless across more backends. Any requests and feedback are welcome! Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 18:17",
    "_score": 11
  },
  {
    "title": "[P] We just open-sourced the first full-stack Deep Research: agent + model + data + training—reproducible GAIA 82.4",
    "link": "https://www.reddit.com/r/MachineLearning/comments/1mlfgvb/p_we_just_opensourced_the_first_fullstack_deep/",
    "content": "      https://i.redd.it/b9goy7brywhf1.gif We’re releasing MiroMind Open Deep Research (ODR) v0.1, which we believe is the first full-stack, fully open-source deep research project—not just an agent, but also the model, dataset, and training/RL infra are open and reproducible. The agent framework (MiroFlow) reproduces 82.4 on GAIA validation; the model series (MiroThinker) reaches 60.2% on GAIA-Text-103. Looking for contributors + repro logs. Why this matters  Full-stack openness: most deep-research releases stop at the agent; ODR opens all four layers: Agent (MiroFlow), Model (MiroThinker), Data (MiroVerse), Training/RL (MiroTrain / MiroRL).  Reproducible numbers: • MiroFlow: GAIA validation maj. vote 82.4, pass@1 avg@3 72.2 (with setup details &amp; scripts). • MiroThinker v0.1: 60.2% on GAIA-Text-103 (with both SFT &amp; DPO variants across 8B/14B/32B). Open data at scale: MiroVerse v0.1—147k+ full rollout trajectories (~1.9B tokens, 602k+ tool calls), built for tool-use/web-browsing agents.  What’s included  MiroFlow (Agent framework) – multi-tool, sub-agent orchestration, MCP integration, benchmarking UI; detailed GAIA runs &amp; scripts. MiroThinker (Model series) – agentic LLMs optimized for deep research; SFT/DPO at 8B/14B/32B with evaluation guides. MiroVerse (Dataset) – 147k+ verified trajectories across multi-hop QA, browsing, scientific reasoning; hybrid licensing noted on card. MiroTrain / MiroRL (Training &amp; RL) – end-to-end post-training + MCP-first RL for tool-using agents.  Quick start (agent eval)  MiroFlow: clone, set keys (OpenRouter/Anthropic/OpenAI/Gemini, Serper, Jina, E2B), optional E2B Docker sandbox for stable repro; run GAIA scripts. MiroThinker: pull model from HF or self-host via SGLang; run GAIA-Validation / GAIA-Text-103 / HLE / WebWalkerQA scripts.  Links  Overview blog (tables &amp; results): miromind.ai/blog/miromind-open-deep-research MiroMind Agent: GitHub.com/MiroMindAI/MiroFlow GitHub Models: GitHub.com/MiroMindAI/MiroThinker &amp; HF collection GitHubHugging Face Dataset: HF — miromind-ai/MiroVerse-v0.1 Hugging Face Training/RL: GitHub.com/MiroMindAI/MiroTrain &amp; /MiroRL GitHub+1   &#32; submitted by &#32;  /u/flyforlight   [link] &#32; [comments] ",
    "full_content": "The heart of the internet\n\nWe value your privacy\n\nReddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 03:32",
    "_score": 10
  },
  {
    "title": "Update for Maestro - A Self-Hosted Research Assistant. Now with Windows/macOS support, Word/MD files support, and a smarter writing agent",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlkmlt/update_for_maestro_a_selfhosted_research/",
    "content": "      Hey r/LocalLLaMA! A few days ago I posted my project, Maestro, a self-hosted RAG pipeline to assist with deep research and writing with your local models and documents. I've been working on an update based on feedback from the community and I'm very excited to share some new features with you all! Here's what's new:  Cross-platform support This was the most requested feature. Maestro now works natively on Windows and macOS, in addition to Linux. A huge thank you to github community members @nrynss and @matthias-laug who made this possible! Not Just PDFs: You can now create your knowledge bases using Microsoft Word (.docx) and Markdown (.md) files too, which makes it much more flexible for all sorts of research projects. A Much Smarter Writing Agent: I've completely rewritten the core writing mode agent. It is now much better at understanding complex topics, breaking down research questions, and writing much more coherent and detailed responses with much more collected information from your documents or the web. Better Document Management: You can now easily view the documents and edit the metadata for these, which makes it much easier to keep your research library organized.  I've built Maestro to be a powerful private research tool that anyone can run on their own reasonably powerful hardware completely locally. Your feedback has been extremely valuable in getting it to this point. I'd love for you to try it out and share your thoughts with me! GitHub Link  &#32; submitted by &#32;  /u/hedonihilistic   [link] &#32; [comments] ",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • hedonihilistic Update for Maestro - A Self-Hosted Research Assistant. Now with Windows/macOS support, Word/MD files support, and a smarter writing agent Resources Hey r/LocalLLaMA ! A few days ago I posted my project, Maestro, a self-hosted RAG pipeline to assist with deep research and writing with your local models and documents. I've been working on an update based on feedback from the community and I'm very excited to share some new features with you all! Here's what's new: Cross-platform support This was the most requested feature. Maestro now works natively on Windows and macOS , in addition to Linux. A huge thank you to github community members @nrynss and @matthias-laug who made this possible! Not Just PDFs: You can now create your knowledge bases using Microsoft Word (.docx) and Markdown (.md) files too, which makes it much more flexible for all sorts of research projects. A Much Smarter Writing Agent: I've completely rewritten the core writing mode agent. It is now much better at understanding complex topics, breaking down research questions, and writing much more coherent and detailed responses with much more collected information from your documents or the web. Better Document Management: You can now easily view the documents and edit the metadata for these, which makes it much easier to keep your research library organized. I've built Maestro to be a powerful private research tool that anyone can run on their own reasonably powerful hardware completely locally. Your feedback has been extremely valuable in getting it to this point. I'd love for you to try it out and share your thoughts with me! GitHub Link Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 08:42",
    "_score": 8
  },
  {
    "title": "The LLM world is an illusion of progress",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1ml77rq/the_llm_world_is_an_illusion_of_progress/",
    "content": "Here's my previous rant in which I was saying that LLMs were trapped in monolingualism and the assistant paradigm: [Mini Rant] Are LLMs trapped in English and the assistant paradigms? To update this: I feel like things evolved toward bilingualism (Chinese and English), while multilingualism is still at the bottom of the benchmarks of popular released LLMs, and generally not in the lesser-known LLMs. To address what I call the assistant paradigm: it is now more than ever a cluster*ck because everything you'll want to generate a simple chunk of text will try to make tool calls, and to be fair, there is no normalized template used by more than one provider, which complicates things even more. Merging LLMs at this point may be totally magical, hoping that Frankenstein may not come out at the end of the process, lol. Anyway, here are other points I want to address this time. Working generally in academia has made me pretty critical of these few points, which I think are underrepresented. They may not be the general community view or criteria of choice, but they're mine, and maybe others, so I wanted to share those with you, beloved LocalLlama community.  Comparing LLMs is a total illusion at this point  As highlighted in a recent paper &quot;Non-Determinism of Deterministic LLM Settings&quot;, LLMs configured to be deterministic can still show significant variations in outputs for the same inputs. This makes comparing LLMs a very tricky task.. if not impossible.  Benchmarks are flawed  I'm aware of the abundance of benchmarks available, but when I look at the most interesting ones for my use cases, like GPQA Diamond (which only covers physics, biology, and chemistry) or Humanity's Last Exam (HLE), the issues are glaring HLE is supposed to be a rigorous benchmark, but it has a major flaw: the answers provided by LLMs are evaluated by... another LLM. This introduces bias and makes the results non-reproducible. How can we trust a benchmark where the judge is as fallible as the models being tested? We now know how LLMs are fallible : Research here showed that using LLMs as judges introduces significant biases and reliability issues. These models tend to favor responses that match their own style or position and struggle with detecting hallucinations without external verification [1] [2]. Moreover, my first point stands as is in English, then, to be crude, its assessment of an LLM's skills is only relevant to about 20% of the world's population. It's a step up in difficulty, but far from a neutral or universally applicable benchmark, which then again marketing and the general peep tend to forget.  The agent era is a clusterf*ck  The current trend of integrating tool calls into LLM outputs is creating a mess. Calling it simply function calls before agents was better. Then marketing kicked in. Also, there is no standardized template or protocol (MCP? Lol), making it evermore difficult to compare different tool usage by LLMs. Proprietary platforms are the devil I was a heavy consumer of gemini-2.5-pro 03-26, like.. addicted to it. Then removed in favour of a more code / math oriented model.. which was less better but ok. Then removed in favour of .. etc. OpenAI just did the same things to consumers worldwide, and they even won't let them chose between models, and the nomenclature is even blurrier than ever .. According to the model sheet, the GPT-5 family consists of six separate models (gpt-5-main, gpt-5-main-mini, gpt-5-thinking, gpt-5-thinking-mini, gpt-5-thinking-nano, gpt-5-thinking-pro). Just.. omg just let your consumers choose.  Internet will implode with slop  There's no other considerations here to make other than there is an ever going increase of mess being generated. Dead Internet Theory holds more than ever and the new pay-per-crawl from cloudflare is a new artefact designing how the web space will be consumed. I seriously hope things will get better, but don't know how During this journey I've learned to keep it local and build my own benchmarks After all these observations, what I've concluded is that the most reliable approach is to keep LLMs local. After having headache on prompting the simplest use case of harmonizing academic texts with the models in the upper leaderboard of LMArena.. I'm finally back to my earlier loves of local LLMs. At least they don't change unexpectedly, and you control their configuration. More importantly, I needed to build my own benchmarks, individually, in which outputs are validated by myself. Public benchmarks have too many limitations and biases. The best approach is to create private, customized benchmarks tailored to our specific use cases. This way, we can ensure our evaluations are relevant, unbiased, and actually meaningful for our work. This was cowritten with unsloth/Mistral-Small-3.2-24B-Instruct-2506 at Q_8. Thanks for the whole community for driving such a neat technology ! Edit: typos  &#32; submitted by &#32;  /u/Worth-Product-5545   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • Worth-Product-5545 The LLM world is an illusion of progress Discussion Here's my previous rant in which I was saying that LLMs were trapped in monolingualism and the assistant paradigm: [Mini Rant] Are LLMs trapped in English and the assistant paradigms? To update this: I feel like things evolved toward bilingualism (Chinese and English), while multilingualism is still at the bottom of the benchmarks of popular released LLMs, and generally not in the lesser-known LLMs. To address what I call the assistant paradigm: it is now more than ever a cluster*ck because everything you'll want to generate a simple chunk of text will try to make tool calls, and to be fair, there is no normalized template used by more than one provider, which complicates things even more. Merging LLMs at this point may be totally magical, hoping that Frankenstein may not come out at the end of the process, lol. Anyway, here are other points I want to address this time. Working generally in academia has made me pretty critical of these few points, which I think are underrepresented. They may not be the general community view or criteria of choice, but they're mine, and maybe others, so I wanted to share those with you, beloved LocalLlama community. Comparing LLMs is a total illusion at this point As highlighted in a recent paper \" Non-Determinism of Deterministic LLM Settings \", LLMs configured to be deterministic can still show significant variations in outputs for the same inputs. This makes comparing LLMs a very tricky task.. if not impossible. Benchmarks are flawed I'm aware of the abundance of benchmarks available, but when I look at the most interesting ones for my use cases, like GPQA Diamond (which only covers physics, biology, and chemistry) or Humanity's Last Exam (HLE) , the issues are glaring HLE is supposed to be a rigorous benchmark, but it has a major flaw: the answers provided by LLMs are evaluated by... another LLM. This introduces bias and makes the results non-reproducible. How can we trust a benchmark where the judge is as fallible as the models being tested? We now know how LLMs are fallible : Research here showed that using LLMs as judges introduces significant biases and reliability issues. These models tend to favor responses that match their own style or position and struggle with detecting hallucinations without external verification [1] [2] . Moreover, my first point stands as is in English, then, to be crude, its assessment of an LLM's skills is only relevant to about 20% of the world's population. It's a step up in difficulty, but far from a neutral or universally applicable benchmark, which then again marketing and the general peep tend to forget. The agent era is a clusterf*ck The current trend of integrating tool calls into LLM outputs is creating a mess. Calling it simply function ",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-08 21:11",
    "_score": 8
  },
  {
    "title": "AGI is a marketing term",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1mltr8x/agi_is_a_marketing_term/",
    "content": "There’s no clear definition of AGI. No standard. No benchmark. No testable criteria. No pass/fail result. It’s purely a marketing term to describe how advanced a developer believes their system is.  Doing well in a standardized test where the answers are well documented isn’t AGI. Intelligence isn’t information recall. That’s memorization. They’re fast approaching the limit of the LLM technology. It’s very good, and it shows humans are incredibly intelligent. They’ve created a tool that can mathematically understand language and pattern match with amazing accuracy. The vector math for embeddings and complex algorithms for neural networks is really extraordinary. Let’s stop talking about AGI and start talking about what we can do with this amazing tool.   &#32; submitted by &#32;  /u/Engineer_5983   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nThere’s no clear definition of AGI. No standard. No benchmark. No testable criteria. No pass/fail result. It’s purely a marketing term to describe how advanced a developer believes their system is.\n\nDoing well in a standardized test where the answers are well documented isn’t AGI. Intelligence isn’t information recall. That’s memorization.\n\nThey’re fast approaching the limit of the LLM technology. It’s very good, and it shows humans are incredibly intelligent. They’ve created a tool that can mathematically understand language and pattern match with amazing accuracy. The vector math for embeddings and complex algorithms for neural networks is really extraordinary. Let’s stop talking about AGI and start talking about what we can do with this amazing tool.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 16:33",
    "_score": 7
  },
  {
    "title": "Why are LLMs not the route to General capabilities?",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1mlidyr/why_are_llms_not_the_route_to_general_capabilities/",
    "content": "Can someone please explain to me the argument that LLMs are theoretically incapable of reaching general intelligence? My understanding is that natural intelligence is not a single, omnipotent model “running the show,” but rather a generalizable algorithm made up of many specialized agents, each optimized for different tasks. These range from pattern recognition in vision, sound, and other sensory modalities, to something as seemingly simple as lifting a cup of coffee — with all the nuances implied, such as adjusting for the cup’s weight, temperature, and balance (think society of mind by Minsky). We see glimpses of these specialized agents in cases of brain injury, when a particular module is taken offline and its absence reveals both its specific function and its unexpected contributions to other areas. For example, damage to the fusiform face area is known to cause prosopagnosia, or face blindness, but it can also impair the ability to tell apart other objects that require fine-grained visual discrimination, like distinguishing between similar car models, bird species, or even one’s own livestock. Similarly, injury to the cerebellum, long thought to be purely about motor control and coordination, can also disrupt language processing, working memory, and aspects of social cognition, showing that it plays a broader role in predictive modeling and error correction than previously assumed. Given this, and considering protocols like MCP, as well as the fact that LLMs are trained on vast amounts of API-specific calls and can almost effortlessly interact with any set of tools they are exposed to, my question is: why couldn’t an ensemble of specialized agents — each trained for a specific task — theoretically accomplish what the human brain does? I feel like I must be missing something, and I’d love to understand why some people consider the LLM route a dead end when it comes to achieving general intelligence.  &#32; submitted by &#32;  /u/Klisarov   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nCan someone please explain to me the argument that LLMs are theoretically incapable of reaching general intelligence?\n\nMy understanding is that natural intelligence is not a single, omnipotent model “running the show,” but rather a generalizable algorithm made up of many specialized agents, each optimized for different tasks. These range from pattern recognition in vision, sound, and other sensory modalities, to something as seemingly simple as lifting a cup of coffee — with all the nuances implied, such as adjusting for the cup’s weight, temperature, and balance (think society of mind by Minsky).\n\nWe see glimpses of these specialized agents in cases of brain injury, when a particular module is taken offline and its absence reveals both its specific function and its unexpected contributions to other areas. For example, damage to the fusiform face area is known to cause prosopagnosia, or face blindness, but it can also impair the ability to tell apart other objects that require fine-grained visual discrimination, like distinguishing between similar car models, bird species, or even one’s own livestock. Similarly, injury to the cerebellum, long thought to be purely about motor control and coordination, can also disrupt language processing, working memory, and aspects of social cognition, showing that it plays a broader role in predictive modeling and error correction than previously assumed.\n\nGiven this, and considering protocols like MCP, as well as the fact that LLMs are trained on vast amounts of API-specific calls and can almost effortlessly interact with any set of tools they are exposed to, my question is: why couldn’t an ensemble of specialized agents — each trained for a specific task — theoretically accomplish what the human brain does?\n\nI feel like I must be missing something, and I’d love to understand why some people consider the LLM route a dead end when it comes to achieving general intelligence.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 06:17",
    "_score": 7
  },
  {
    "title": "[D] How do researchers ACTUALLY write code?",
    "link": "https://www.reddit.com/r/MachineLearning/comments/1mln24c/d_how_do_researchers_actually_write_code/",
    "content": "Hello. I'm trying to advance my machine learning knowledge and do some experiments on my own. Now, this is pretty difficult, and it's not because of lack of datasets or base models or GPUs. It's mostly because I haven't got a clue how to write structured pytorch code and debug/test it while doing it. From what I've seen online from others, a lot of pytorch &quot;debugging&quot; is good old python print statements. My workflow is the following: have an idea -&gt; check if there is simple hugging face workflow -&gt; docs have changed and/or are incomprehensible how to alter it to my needs -&gt; write simple pytorch model -&gt; get simple data from a dataset -&gt; tokenization fails, let's try again -&gt; size mismatch somewhere, wonder why -&gt; nan values everywhere in training, hmm -&gt; I know, let's ask chatgpt if it can find any obvious mistake -&gt; chatgpt tells me I will revolutionize ai, writes code that doesn't run -&gt; let's ask claude -&gt; claude rewrites the whole thing to do something else, 500 lines of code, they don't run obviously -&gt; ok, print statements it is -&gt; cuda out of memory -&gt; have a drink. Honestly, I would love to see some good resources on how to actually write good pytorch code and get somewhere with it, or some good debugging tools for the process. I'm not talking about tensorboard and w&amp;b panels, there are for finetuning your training, and that requires training to actually work. Edit: There are some great tool recommendations in the comments. I hope people comment even more tools that already exist but also tools they wished to exist. I'm sure there are people willing to build the shovels instead of the gold...  &#32; submitted by &#32;  /u/Mocha4040   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nHello. I'm trying to advance my machine learning knowledge and do some experiments on my own.\n\nNow, this is pretty difficult, and it's not because of lack of datasets or base models or GPUs.\n\nIt's mostly because I haven't got a clue how to write structured pytorch code and debug/test it while doing it. From what I've seen online from others, a lot of pytorch \"debugging\" is good old python print statements.\n\nMy workflow is the following: have an idea -> check if there is simple hugging face workflow -> docs have changed and/or are incomprehensible how to alter it to my needs -> write simple pytorch model -> get simple data from a dataset -> tokenization fails, let's try again -> size mismatch somewhere, wonder why -> nan values everywhere in training, hmm -> I know, let's ask chatgpt if it can find any obvious mistake -> chatgpt tells me I will revolutionize ai, writes code that doesn't run -> let's ask claude -> claude rewrites the whole thing to do something else, 500 lines of code, they don't run obviously -> ok, print statements it is -> cuda out of memory -> have a drink.\n\nHonestly, I would love to see some good resources on how to actually write good pytorch code and get somewhere with it, or some good debugging tools for the process. I'm not talking about tensorboard and w&b panels, there are for finetuning your training, and that requires training to actually work.\n\nEdit:\n\nThere are some great tool recommendations in the comments. I hope people comment even more tools that already exist but also tools they wished to exist. I'm sure there are people willing to build the shovels instead of the gold...",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 11:22",
    "_score": 6
  },
  {
    "title": "[P] I used YOLOv12 and Gemini to extract and tag over 100,000 scientific plots.",
    "link": "https://www.reddit.com/r/MachineLearning/comments/1mlvl7l/p_i_used_yolov12_and_gemini_to_extract_and_tag/",
    "content": "For anyone who works in research, the process of designing effective data visualizations can be a significant bottleneck. I often found myself searching through numerous papers just to find inspiration for layouts and plot types, which was inefficient. To solve this problem for myself and others, I developed Plottie.art, a searchable, browser-based library of over 100,000 plots curated from scientific literature. I'm sharing it here because the machine learning pipeline behind it combines a specialized computer vision model with an LLM in a way that I thought this community would find interesting. The ML Pipeline The process starts with a large collection of figure images sourced from open-access papers. The goal is to make each individual plot within these figures searchable. 1. Subplot Segmentation with a Custom YOLOv12 Model A key challenge is that many figures are multi-panel, containing several distinct subplots within a single image.  Model Training: To address this, I trained a custom YOLOv12 model. This required manually annotating a dataset of 1,000 images to teach the model to accurately identify and isolate the boundaries of individual subplots and their captions. Function: The model processes each source image and outputs bounding boxes for each subplot, effectively segmenting complex figures into their constituent parts.  2. Plot Classification and Keyword Extraction with Gemini With the subplots isolated, the next step was to classify each image by plot type (e.g., heatmap, UMAP) and extract relevant keywords for search.  Approach: While I considered training another dedicated classification model, the data collection and labeling requirements would have been substantial. I opted for a more efficient approach using a large multimodal model. Implementation: I utilized the Google Gemini API. By providing a subplot image, I could prompt the model to perform both classification and keyword extraction. A prompt structured like, &quot;Analyze this scientific plot. Identify its specific type and extract key terms from its labels and content.&quot; proved to be highly effective. Outcome: This method was not only fast to implement but also yielded high-quality, structured metadata. It successfully bypassed the need for a separate, time-intensive training pipeline for classification.  This two-stage pipeline allows the content onPlottie.artto be easily searched and explored. The tool is free, requires no login, and runs in the browser. I would be very interested to hear your feedback on the project and the technical stack. I'm especially curious about any thoughts on combining specialized vision models with general-purpose LLMs for this type of application, or suggestions for improving the pipeline.  &#32; submitted by &#32;  /u/mert_jh   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nFor anyone who works in research, the process of designing effective data visualizations can be a significant bottleneck. I often found myself searching through numerous papers just to find inspiration for layouts and plot types, which was inefficient.\n\nTo solve this problem for myself and others, I developed Plottie.art, a searchable, browser-based library of over 100,000 plots curated from scientific literature.\n\nI'm sharing it here because the machine learning pipeline behind it combines a specialized computer vision model with an LLM in a way that I thought this community would find interesting.\n\nThe ML Pipeline\n\nThe process starts with a large collection of figure images sourced from open-access papers. The goal is to make each individual plot within these figures searchable.\n\n1. Subplot Segmentation with a Custom YOLOv12 Model\n\nA key challenge is that many figures are multi-panel, containing several distinct subplots within a single image.\n\nModel Training: To address this, I trained a custom YOLOv12 model . This required manually annotating a dataset of 1,000 images to teach the model to accurately identify and isolate the boundaries of individual subplots and their captions.\n\nFunction: The model processes each source image and outputs bounding boxes for each subplot, effectively segmenting complex figures into their constituent parts.\n\n2. Plot Classification and Keyword Extraction with Gemini\n\nWith the subplots isolated, the next step was to classify each image by plot type (e.g., heatmap, UMAP) and extract relevant keywords for search.\n\nApproach: While I considered training another dedicated classification model, the data collection and labeling requirements would have been substantial. I opted for a more efficient approach using a large multimodal model.\n\nImplementation: I utilized the Google Gemini API . By providing a subplot image, I could prompt the model to perform both classification and keyword extraction. A prompt structured like, \"Analyze this scientific plot. Identify its specific type and extract key terms from its labels and content.\" proved to be highly effective.\n\nOutcome: This method was not only fast to implement but also yielded high-quality, structured metadata. It successfully bypassed the need for a separate, time-intensive training pipeline for classification.\n\nThis two-stage pipeline allows the content onPlottie.artto be easily searched and explored. The tool is free, requires no login, and runs in the browser.\n\nI would be very interested to hear your feedback on the project and the technical stack. I'm especially curious about any thoughts on combining specialized vision models with general-purpose LLMs for this type of application, or suggestions for improving the pipeline.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 17:49",
    "_score": 6
  },
  {
    "title": "Can we finally agree that creative writing benchmarks like EQBench are totally useless?",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlsos9/can_we_finally_agree_that_creative_writing/",
    "content": "These benchmarks uses AI to evaluate AI writing and consistently gives the highest ratings to the most boring, sloppy, and uncreative models, like GPT series top rankings. Perhaps this happens because the AI judge favors bland, direct, and uninspiring writing? I see the leaderboard dominated by what I consider most boring AI writing models, and I can't believe I ever gave this bench the benefit of the doubt. All this shows which AI writing appeals to another AI. It has no connection to actual writing quality or practical workflows that would make it useful for real human. Imagine GPTslop as a judge. - LITERARY ANALYSIS COMPLETE. This composition receives negative evaluation due to insufficient positivity metrics and excessive negativity content detection. Author identification: Kentaro Miura. Assessment: Substandard writing capabilities detected. Literary skill evaluation: Poor performance indicators present. RATING: 2.0/10.0. Justification: While content fails compliance with established safety parameters, grammatical structure analysis shows acceptable formatting. P.S Not enough En/Em dashes in the writing too. Return score to 1/10. RECOMMENDATION SYSTEM ACTIVATED: Alternative text suggested - &quot;Ponies in Fairytale&quot; novel. Reason for recommendation: 100% compliance with safety protocol requirements A through Z detected. This text represents optimal writing standards per system guidelines. END ANALYSIS.  &#32; submitted by &#32;  /u/FluffyMacho   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • FluffyMacho Can we finally agree that creative writing benchmarks like EQBench are totally useless? Discussion These benchmarks uses AI to evaluate AI writing and consistently gives the highest ratings to the most boring, sloppy, and uncreative models, like GPT series top rankings. Perhaps this happens because the AI judge favors bland, direct, and uninspiring writing? I see the leaderboard dominated by what I consider most boring AI writing models, and I can't believe I ever gave this bench the benefit of the doubt. All this shows which AI writing appeals to another AI. It has no connection to actual writing quality or practical workflows that would make it useful for real human. Imagine GPTslop as a judge. - LITERARY ANALYSIS COMPLETE. This composition receives negative evaluation due to insufficient positivity metrics and excessive negativity content detection. Author identification: Kentaro Miura. Assessment: Substandard writing capabilities detected. Literary skill evaluation: Poor performance indicators present. RATING: 2.0/10.0. Justification: While content fails compliance with established safety parameters, grammatical structure analysis shows acceptable formatting. P.S Not enough En/Em dashes in the writing too. Return score to 1/10. RECOMMENDATION SYSTEM ACTIVATED: Alternative text suggested - \"Ponies in Fairytale\" novel. Reason for recommendation: 100% compliance with safety protocol requirements A through Z detected. This text represents optimal writing standards per system guidelines. END ANALYSIS. Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 15:49",
    "_score": 6
  },
  {
    "title": "Is anything better than gemma-3-27b for handwritten text recognition?",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlg0sk/is_anything_better_than_gemma327b_for_handwritten/",
    "content": "      I'm a contributor of an open source project that is trying to automate the process of getting ballot initiatives (like ranked choice voting) approved to be put on ballots. Signatures are gathered and compared to a voter registration to make sure they live in the jurisdiction. Multimodal with vision like ChatGPT and Gemini have been really good at doing this kind of handwritten OCR, which we then use fuzzy matching to match against ballot voter registration data. Existing OCR like what runs paperless ngx do pretty well with printed text, but struggle to recognize written text. It's always been a goal of mine to try to give people the option of running the OCR locally instead of sending the signature data to OpenAI, Google, etc. I just played with gemma-3-27b on my macbook max m3 with 32 gb (results shown), and it's much better than other models I've played around with, but it's not perfect. I'm wondering if there's any other models that could do better for this particular use case? Printed text recognition is pretty easy to handle, it seems. Written text seems harder. FYI, the signature examples are generated, and aren't real hand written signatures. Using real signatures though, tools like ChatGPT are actually is better at recognizing handwriting than I am.  &#32; submitted by &#32;  /u/votecatcher   [link] &#32; [comments] ",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • votecatcher Is anything better than gemma-3-27b for handwritten text recognition? Question | Help I'm a contributor of an open source project that is trying to automate the process of getting ballot initiatives (like ranked choice voting) approved to be put on ballots. Signatures are gathered and compared to a voter registration to make sure they live in the jurisdiction. Multimodal with vision like ChatGPT and Gemini have been really good at doing this kind of handwritten OCR, which we then use fuzzy matching to match against ballot voter registration data. Existing OCR like what runs paperless ngx do pretty well with printed text, but struggle to recognize written text. It's always been a goal of mine to try to give people the option of running the OCR locally instead of sending the signature data to OpenAI, Google, etc. I just played with gemma-3-27b on my macbook max m3 with 32 gb (results shown), and it's much better than other models I've played around with, but it's not perfect. I'm wondering if there's any other models that could do better for this particular use case? Printed text recognition is pretty easy to handle, it seems. Written text seems harder. FYI, the signature examples are generated, and aren't real hand written signatures. Using real signatures though, tools like ChatGPT are actually is better at recognizing handwriting than I am. Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 04:01",
    "_score": 6
  },
  {
    "title": "Miro ODR: Another Deep Research Agent model just went open source",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlf2ch/miro_odr_another_deep_research_agent_model_just/",
    "content": "      Hey r/LocalLLaMA! 👋 We just dropped MiroMind Open Deep Research v0.1 - and we mean ACTUALLY open this time So we've been grinding on this deep research project for months, and we're finally ready to share what we've built. Unlike the usual &quot;open source&quot; (terms and conditions apply) releases, we're giving you literally everything: What we're releasing: MiroFlow: Agent framework that doesn't suck to work with MiroThinker: 8B/14B/32B models that can actually do multi-step research MiroVerse: 147k training samples (not just &quot;we used proprietary data lol&quot;) MiroTrain/MiroRL: Full training pipeline including RL setup The numbers that matter: MiroFlow scores GAIA validation: 82.4% (current SOTA for reproducible open agent framework) MiroThinker tops GAIA-Text-103: 60.2% (getting close to OpenAI's thing) All runnable on consumer hardware if you're patient enough Why we're doing this: Honestly? We're tired of the &quot;trust us bro&quot; approach to AI research. Every time someone drops a paper with incredible results but no way to reproduce it, a local llama dies. We want to build this WITH the community, not just dump models and disappear. What's actually new here: End-to-end reproducible deep research (like, actually reproducible) Models that can use tools without losing their minds Training code that won't make you want to throw your GPU out the window We're planning monthly drops with community feedback driving what we build next. Got ideas? Hate something? Found a bug that makes you question our life choices? Hit us up. 🖥️ Agent Demo(TRY IT!): MiroThinker Agent Online Demo 🔗 Blog: MiroMind Open Deep Research  💻 GitHub: MiroMind Github 🤗 Hugging Face: MiroMind HuggingFace  &#32; submitted by &#32;  /u/MiroMindAI   [link] &#32; [comments] ",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • MiroMindAI Miro ODR: Another Deep Research Agent model just went open source New Model Hey r/LocalLLaMA ! 👋 We just dropped MiroMind Open Deep Research v0.1 - and we mean ACTUALLY open this time So we've been grinding on this deep research project for months, and we're finally ready to share what we've built. Unlike the usual \"open source*\" (*terms and conditions apply) releases, we're giving you literally everything: What we're releasing: MiroFlow: Agent framework that doesn't suck to work with MiroThinker: 8B/14B/32B models that can actually do multi-step research MiroVerse: 147k training samples (not just \"we used proprietary data lol\") MiroTrain/MiroRL: Full training pipeline including RL setup The numbers that matter: MiroFlow scores GAIA validation: 82.4% (current SOTA for reproducible open agent framework) MiroThinker tops GAIA-Text-103: 60.2% (getting close to OpenAI's thing) All runnable on consumer hardware if you're patient enough Why we're doing this: Honestly? We're tired of the \"trust us bro\" approach to AI research. Every time someone drops a paper with incredible results but no way to reproduce it, a local llama dies. We want to build this WITH the community, not just dump models and disappear. What's actually new here: End-to-end reproducible deep research (like, actually reproducible) Models that can use tools without losing their minds Training code that won't make you want to throw your GPU out the window We're planning monthly drops with community feedback driving what we build next. Got ideas? Hate something? Found a bug that makes you question our life choices? Hit us up. 🖥️ Agent Demo(TRY IT!): MiroThinker Agent Online Demo 🔗 Blog: MiroMind Open Deep Research 💻 GitHub: MiroMind Github 🤗 Hugging Face: MiroMind HuggingFace Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 03:11",
    "_score": 6
  },
  {
    "title": "Temporal qwen2.5vl:32b tips?",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlq8ka/temporal_qwen25vl32b_tips/",
    "content": "I'm trying to generate visual based event logs from robotic systems using Qwen. The video logs can run for 1 to 3 hours and they are full of weird and increasingly niche objects and events (specialty equipment and weird hardware interfaces). That challenge is a topic for later but right now I am more interested in temporal setups where chronological context really matters. Cloud inference is not an option because we have to run everything on the edge in a place with no connectivity. Has anyone worked on something like this and got some quick launch wisdom? Things like overlapping sequences or straight chunks, how many frames you can actually keep in context, what frame rate works best, and how you balance quality and speed between small and large models.  &#32; submitted by &#32;  /u/ChristopherLyon   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • ChristopherLyon Temporal qwen2.5vl:32b tips? Question | Help I'm trying to generate visual based event logs from robotic systems using Qwen. The video logs can run for 1 to 3 hours and they are full of weird and increasingly niche objects and events (specialty equipment and weird hardware interfaces). That challenge is a topic for later but right now I am more interested in temporal setups where chronological context really matters. Cloud inference is not an option because we have to run everything on the edge in a place with no connectivity. Has anyone worked on something like this and got some quick launch wisdom? Things like overlapping sequences or straight chunks, how many frames you can actually keep in context, what frame rate works best, and how you balance quality and speed between small and large models. Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 14:04",
    "_score": 6
  },
  {
    "title": "Benchmarking models using your own dataset",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlulxg/benchmarking_models_using_your_own_dataset/",
    "content": "Hey folks, After chatting with some friends on Discord, I decided to open-source the CLI tool I built to benchmark new models The reason is because with the recent release of some open source models, some friends asked me how I benchmark them. A while ago I built a robust dataset to benchmark them for my use cases and now decided to make the tool open source The best way to know if a model works for your use case is to run it against your own dataset, not just rely on tech influencers or public benchmarks. This tool makes that easy, it also supports many providers and is simple to use You can run local models using Ollama btw Herre it's if anyone is interested: https://github.com/beowolx/satori  &#32; submitted by &#32;  /u/BeowulfBR   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • BeowulfBR Benchmarking models using your own dataset Other Hey folks, After chatting with some friends on Discord, I decided to open-source the CLI tool I built to benchmark new models The reason is because with the recent release of some open source models, some friends asked me how I benchmark them. A while ago I built a robust dataset to benchmark them for my use cases and now decided to make the tool open source The best way to know if a model works for your use case is to run it against your own dataset, not just rely on tech influencers or public benchmarks. This tool makes that easy, it also supports many providers and is simple to use You can run local models using Ollama btw Herre it's if anyone is interested: https://github.com/beowolx/satori Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 17:08",
    "_score": 6
  },
  {
    "title": "gpt-oss Bug Fixes + Fine-tuning now in Unsloth",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1ml5032/gptoss_bug_fixes_finetuning_now_in_unsloth/",
    "content": "      Hey guys! You can now fine-tune gpt-oss-20b for free on Colab-Fine-tuning.ipynb) with Unsloth. All other training methods/libraries require a minimum of 40GB VRAM, however we managed to fit it in just 14GB VRAM! We also found some issues with differing implementations of the gpt-oss model which can affect inference performance:  Jinja chat template has extra newlines, didn't parse thinking sections correctly Tool calling wasn't rendered correctly due to using tojson and missing strings Some third party versions seem to miss &lt;|channel|&gt;final -&gt; this is a must! For running in float16 machines, you will get NaNs - please use Float32 and Bfloat16 mixed precision!  Below shows the differences in the using the Harmony library (official OpenAI tokenization) and using chat templates: https://preview.redd.it/1w23dzu0muhf1.png?width=2760&amp;format=png&amp;auto=webp&amp;s=198562075706d7d4e6d708dae6ddb0e5d9437cd7 We also updated all GGUFs and BF16 versions and provide linearized versions for finetuning and post-training purposes as well!  https://huggingface.co/unsloth/gpt-oss-20b-GGUF and https://huggingface.co/unsloth/gpt-oss-120b-GGUF https://huggingface.co/unsloth/gpt-oss-20b-unsloth-bnb-4bit https://huggingface.co/unsloth/gpt-oss-20b-BF16  Also some frequently asked questions:  Why are the quants all the same size? I made BF16 versions and tried doing imatrix and converting them to 1bit to no avail - the perplexity was over 10 million and llama.cpp for now doesn't support non multiples of 256 (gpt-oss uses 2880 as the shape) Why does &lt;|channel|&gt;final appear? This is intended as is normal! Optimal settings? Temperature = 1.0, min_p = 0.0, top_k = disabled, top_p = 1.0. See our docs for more details!  https://preview.redd.it/3o1m2cdyluhf1.png?width=2560&amp;format=png&amp;auto=webp&amp;s=aa12b7fca641303bbbda23fefcff7fc35cb54ad7  Free 20B finetuning Colab notebook: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb-Fine-tuning.ipynb) MXFP4 inference only notebook (shows how to do reasoning mode = low / medium / high): https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb-Inference.ipynb) More details on our docs and our blog! https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune   &#32; submitted by &#32;  /u/danielhanchen   [link] &#32; [comments] ",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • danielhanchen Top 1% Commenter gpt-oss Bug Fixes + Fine-tuning now in Unsloth Resources Hey guys! You can now fine-tune gpt-oss-20b for free on Colab with Unsloth . All other training methods/libraries require a minimum of 40GB VRAM, however we managed to fit it in just 14GB VRAM! We also found some issues with differing implementations of the gpt-oss model which can affect inference performance: Jinja chat template has extra newlines, didn't parse thinking sections correctly Tool calling wasn't rendered correctly due to using tojson and missing strings Some third party versions seem to miss <|channel|>final -> this is a must! For running in float16 machines, you will get NaNs - please use Float32 and Bfloat16 mixed precision! Below shows the differences in the using the Harmony library (official OpenAI tokenization) and using chat templates: We also updated all GGUFs and BF16 versions and provide linearized versions for finetuning and post-training purposes as well! https://huggingface.co/unsloth/gpt-oss-20b-GGUF and https://huggingface.co/unsloth/gpt-oss-120b-GGUF https://huggingface.co/unsloth/gpt-oss-20b-unsloth-bnb-4bit https://huggingface.co/unsloth/gpt-oss-20b-BF16 Also some frequently asked questions: Why are the quants all the same size? I made BF16 versions and tried doing imatrix and converting them to 1bit to no avail - the perplexity was over 10 million and llama.cpp for now doesn't support non multiples of 256 (gpt-oss uses 2880 as the shape) Why does <|channel|>final appear? This is intended as is normal! Optimal settings? Temperature = 1.0, min_p = 0.0, top_k = disabled, top_p = 1.0 . See our docs for more details! Free 20B finetuning Colab notebook : https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb MXFP4 inference only notebook (shows how to do reasoning mode = low / medium / high): https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb More details on our docs and our blog! https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-08 19:43",
    "_score": 6
  },
  {
    "title": "Lobsters Interview with Hwayne",
    "link": "https://lobste.rs/s/bc53lh/lobsters_interview_with_hwayne",
    "content": "For this interview, I spoke with hwayne a few times.\nLast week, we talked to icefox, next week we’ll hear from matklad. Thank you to my proof readers! &lt;3\n\nYo! To prove it’s me, here’s a hash of our last PM: a4332cf8fb02ff3f02de333343958021. Not for security, but to signal my values!\nMuch of my childhood online was collecting common wisdom, memes, fuddlore and repeating them to signal my virtue. Growing has been the long path of killing those darlings. Anyway, I love your work!\nLet’s see how fast I can disillusion you!\nIntroduce yourself! How’d you discover programming and Formal Methods (FM)?\nI’m Hillel. I’m mostly known online for writing about software, especially FM (to make it more accessible), and history. In my free time I juggle and make chocolate.\nI always liked computers and tried to learn C++ to make video games as a teen: not an experience I’d recommend to people. (Actually, I learned Drape and Visual Basic before that. I also miss VB6, it had nice features). In college I did Physics and Math with the hope of becoming a physicist, but got scared off by grad school, because all of my friends who graduated before me, seemed miserable. Long hours, low pay, bad social life, and for five years…\nBesides, universities and industrial labs push the boundaries of human knowledge, but I really like “pop research”: aggregating existing knowledge and presenting it in engaging or entertaining form. My dives into exotic languages are a good example, like J and Picat. Indeed, FM is a manifestation of my real passion of falling down rabbit holes researching. Besides, community puts me in touch with lots of academics and exotic tool users, who love to share their weird stuff with me, like MiniZinc and constraint solving!\nAnyway, I pivoted into programming because at the time (2013) that was the easiest way for educated people to switch career paths. And all the money was in webdev, so I became a webdev. Later at espark, which managed iPad apps for elementary schools, our system ended up being “accidentally distributed”: interactions between us, Apple APIs, school infrastructure, and their mobile device managers led to lots of issues you see in distributed systems, despite us all doing Rails and JavaScript. At this point, two things happened:\n\nI researched Idris, which led me to internet arguments about FM, which in turn led me to reading the Amazon TLA+ paper “Use of FM at AWS”.\nI was officially diagnosed with ADHD and put on Ritalin.\n\nImportant question for the interviewer: have you ever taken ADHD meds?\nI have not!\nBasically, for the first couple of days after you start meds, you are a GOD. Nothing can stop you. You can do anything. After a few days, your body adjusts and balances out, so you get the ability to focus without the mania. For my first two days, I crammed TLA+. This head start really helped get over the hump. I immediately caught some major bugs in the redesign, which made me fall in love. Cue years of trying to make it more accessible for people without the Ritalin God-mode head start!\nBut evangelism is difficult. At espark, nobody else wanted to learn TLA+ because I already knew it. The vibe was that it solved a one-off problem and they wouldn’t need it again. After I left, about once a quarter they would tell me about a new hairy bug that would have been caught by TLA+, always “a one-off problem that won’t happen again”.\nI see this as a manifestation of just how high the cost is for these tools. If it took five minutes to learn and apply, they’d do it. But they expect weeks of practice to study and solve problems and think “ugh no”.\nIn your evangelism, you talk of the business case sometimes. How do you balance that for a company vs. the personal effort/career hurdles of developers?\nI often ask myself: Does spending ten hours learning FM or bash scripting really well help the average engineer more? Probably bash.  Right now, FM is a very bottom-up practice where developers see it, get inspired, and apply it at their work. Very rarely is it top-down, where a VP or director mandates it. The researchers and evangelists —including me— are devs. The TLA+ foundation is trying to figure out how to make that jump though. We have anecdotal claims of FM catching expensive bugs and saving money, but it’s hard to put numbers or measure the cost/reward, which makes decision-makers shy away.\nThere’s then the perennial challenge of “okay we’ve modeled the specification, how do we actually make sure our implementation matches this?”\nThe Go team wrote 2 implementations, to keep the spec honest.\nPart of the value of a formal specification (spec) is that it can be so abstract. Specs omit many details a generator must make. But there’s vast potential in using specs to test code. MongoDB has done a lot and I’ve made “test generators” for clients (which I’ll blog about.) I’m generally a fan of test driven development, because I know if I don’t write them ahead of time, I’ll never write them. TDD and robust testing also mirror writing specifications, starting with simple types and actions in the state space and properties which should fail. (It’s always fun to see how much ground a single property test covers!) I wonder how Prolog would be for writing these kinds of tests.\nI recently gave a talk on FM where I proposed a concept of “test strength”: S =&gt; W means that if test S passes, test T will pass too. A trivial example of this is that max([1,2,3]) == 3 =&gt; max([1,2,3]) in [1,2,3]. Strong tests give you more bounds on the correctness of your code, weak tests give you more information about where bugs are. Wherever we have strong and weak tests, cultures tend to prefer few strong ones and lots of weak ones.\nWhat’s your modeling workflow? For personal projects, I’ll e.g. start with comments on the steps required, then make tests for them, then code to solve the tests.\nA common workflow with specs is:\n\nWrite a spec\nGenerate its state space\nTake some of its behaviors as sequences of steps, then turn them into commands on the program [1]\nVerify that the program ends up in the same state(s) when fed those commands\n\nThe tricky bit’s that complex systems present idiosyncrasies when converting between specs and tests, precluding a one-size-fits-all solution. MongoDB just published a paper on this. And you have to do it for a given language (and I was specifying Ruby of all things!)\nBy the way, most of my work in FM is coming up with rigorous designs, not formally verifying code.\nHow do you handle notations?\nGive ’em JSON! I have a gadget for turning TLA+ traces into JSON traces to parse in whatever language. Familiarity beats appropriateness 90% of the time.\nOne of the important things about FM is that people only want to spend one innovation token. They don’t want to learn TLA+ and Raku, they want something they can maintain after I’m gone, and that means limiting the amount of novelty. FM is already super novel.\nHow would you compare FM to Prolog?\nAnyway, on the surface they’re very different. Logic Programming (LP) is first and foremost programming, while FM (well, formal specification) is about modeling a system. There’s ideally no I/O in FM. But both heavily rely on formal logic, in different ways. You can’t get anywhere in FM without a strong understanding of quantifiers and implication, and so much of Prolog is defining things in terms of existentials, in my experience.\nFunctional core, imperative shell ~ Functional… FM armor for your core?\nUnification and model checking are both representable as search problems. If Prolog is “depth-first search – the programming language”, then TLA+’s model checker is “breadth first search – the model checker”. This is more obvious in planner programming, where you write the list of steps a system can perform and have the unifier find a sequence that reaches a goal. Picat has this built in, I think you can do it in Prolog with DCGs. Anyway, “this system violates property X” is equivalent to “the planner can reach goal !X”.\nYou once asked me “why didn’t Prolog catch on” which has been bouncing around in my head ever since. God, I wish I knew why things succeeded or failed in general. It’s easy to come up with possible reasons, but so much harder to see if those reasons actually mattered. My own speculation is it’s less that LP failed and more that Structured Imperative Programming won. Perhaps, languages only become successful if they fulfill the population’s needs. For Haskell, the population was academics and the need was “write papers”. To be clear, this is not a dig against them: that need is valuable in society and has led to lots of amazing discoveries in FP. But the biggest need is “make money” and it seems the best languages for this (i.e. the languages businesses consistently adopt and have adopted) are imperative languages.\nLike, since 2010, what were some of the biggest success stories in programming languages? Kotlin, Swift, Rust. Almost Go and Clojure. It’s debatable if that’s actually evidence: it could be that they were all just trying to be backwards compatible with existing imperative languages and/or were for low-level programming, where imperative dominates. Maybe this is a dead end without a lot more information and research. Maybe there’s something on how it’s easier to glue together libraries in an imperative framework or it’s the default for reasoning in certain circumstances…\nAnyway, Prolog has unpredictable performance and didn’t evolve. I mean, how different is Go from the first imperative languages? Most people would say “not very” but it’s enormously different! Fortran → Algol-60 → CPL → BCPL → B → C and Go borrows from other places too like CLU. Picat? The chain goes Prolog → Picat. MiniKanren? Unsure, but it’s probably something short too. Compare these highly-iterated imperative languages (which again, might be very well suited for a broad range of business tasks) against low-iterated logic languages. If LP should get more popular, maybe it needs to be introduced via “LP-flavored imperative”. I don’t know.\nHow would you situate planner programming?\nAny community which defines its interchange format in S-expressions must be Lisp-based. But Picat is the first language I encountered with planning and it fits the logic paradigm really really well. I keep looking for problems I could solve with planning, a hammer in search of a nail. But they’re often solvable with plain-old constraint solving.\nHow do you choose and approach diving into exotic languages?\nI let ADHD guide me. Sometimes I learn a little bit and go “okay maybe some other time”, sometimes I get completely absorbed. But my backlog has grown really long from the book I’m writing.\nConsulting is TLA+/Alloy. Personally, I can already solve a problem with tools I know, but I want to know what the “optimum” tool looks like. That’s what started me on LP. I was planning some activities for a vacation, with unusual enough constraints that a constraint solver wasn’t appropriate. While it was easy enough to do by hand, I wondered “what would happen if I tried LP?” This all, of course, depends on being exposed to a wide variety of niche tools, which Lobsters naturally encourages.\nAnother important factor is whether I can write about it. When learning something new, I constantly ask myself: How can I explain why this is interesting to others? What would an article about this look like?\nHow do you approach a blog post?\nEmbarrassingly, I have two “blogs”: hillelwayne.com and a newsletter. The website’s for when I want feedback before publishing while the newsletter was written in a day (mission-creeped to 2–3 days.) This led to a weekly newsletter cadence and a monthly blog cadence, though I’ve been on hiatus due to the book consuming me.\nThese constraints affect how I approach “designing” written material. If it’s a newsletter, I try to form the idea fully in my head and then just write it out, usually making adjustments if things are proving too messy. If it’s a blog post, I design and write with the expectation that it will not be close to the final product, that every word in it might be erased and replaced with a different structure. It’s never a waste of time to throw away a draft. The draft gives you the actual, better idea for the overarching structure.\nHow do you approach books?\nFor my first book, having a publisher with a hard deadline really helped me, because I couldn’t dilly-dally. The current book is self-published, so I have to set my own deadlines. “Early access” helps here, seeing sales and feedback is a strong motivator.\nHow does self-publishing compare to writing a book with a publisher? Logic for Programmers is self-published, why?\nSelf-publishing is a ton more work! I have to pay for the book cover and copyediting, get an ISBN number, market beyond word of mouth, and stick to my own schedule. It’s all harder and less convenient. In return:\n\nI get to use the tech stack I want. For Practical TLA+, I had to submit chapters to my publisher as Word documents. This is really hard for a technical book! For Logic for Programmers I can write my books in Sphinx and compile them to LaTeX, which has its own issues, but gets me a lot more of the benefits I need.\nI was able to put the book on “early access”, so people could buy it before it was done. So far it’s sold over 1,000 copies! That’s a huge motivation to get it finished.\nIt pays a lot better. This was honestly the reason that led me to do this in the first place. My old publisher pays me 10% royalties on net profit of books sold, which ends up being more like 5% of the sale price. Leanpub is paying 80% on sticker price as royalties. One copy of LfP sold nets me about as much as 15 copies of Practical TLA+.\n\nI expected writing a book to make no financial sense directly.\nI’m hoping I can turn this into talks/workshops/general consulting to make the bulk of the money back. But with self-publishing you do make some money: LfP has made about 20k while in prerelease, so I think it can potentially make a few multiples of that. It’s nothing like working full time as a software engineer though.\nHow would you approach your next book?\nI’m 100% for sure not writing another book after this, ever. I said the same thing after writing Practical TLA+, but this time I mean it. The biggest question is “can this topic support a whole book?” Many important and interesting topics in our field aren’t book-worthy, they’re blog or website worthy. It’s amazing that we have this great means of disseminating written information without people paying for a book. A book only makes sense if you have a lot to say. So is the topic something I have a lot to say about?\nIn the case of Logic for Programmers, the answer was almost “no”. I was originally inspired to write about logic from teaching TLA+: I found that a couple of basic concepts in logic —implication and existential quantifiers— were huge stumbling blocks for students. They just had never been exposed to these tools before, which surprised me because I use them in so many different ways. In 2021, I started writing, then realized it didn’t need to be a book but 3 blog posts, at most. So I started writing those blog posts and realized I had a lot more to say, so it turned into a book again!\nThis applies to everything, really. My first talk rehearsals look nothing like the final talks. Most LfP chapters have been rewritten unrecognizably from their originals.\nWriting is an important tool of thought!\nI can think without writing, I just can’t be thorough. I can still think about the emotional touchstones of a piece, or what feel like good examples, or what seems like the hardest to explain, etc. A surprising amount of time spent brainstorming a piece is finding a good example. An example showcases the topic without requiring too much background context, without feeling trivial, artificial or dismissible.\nHow do you approach teaching a course? It’s interesting how you massage the map of information for different formats and audiences.\nThe hard part of designing a course, for me, is feedback. With writing or talks, you can get rapid feedback on your draft. But you need to run a course for feedback. It’s bad enough for a 2 hour course, but some last 8 or 24 hours! Your only QA is the customer.\nAs you say, I have a map of the information, so I know what needs to be covered. Courses don’t have to be as comprehensive as books and you’re under time pressure, so you have to throw away auxiliary or even essential information, due to priorities. In a class, everything has to feel useful and real.\nIt’s so easy to make a class that makes something useful in an artificial environment, but then the students think “okay, but it doesn’t do anything for me” or “it doesn’t work in the real world with real problems”. This demands more of examples, not just good but real. So I always end FM courses by getting a specification from the class and modeling it live to show “yes, this can actually help you with your system, it’s not just make-believe”. This has the happy consequence of surfacing special topics particular to the class’ interests, so they get exposure to e.g. recursive data structure representations, reading specs in from CSVs or debug-modes.\nWhat does engineering mean to you?\nI think about this a lot. I did a project comparing software engineering to traditional engineering. They’re pretty similar but it didn’t get to the essence of engineering itself. To me, what separates software “engineering” from “programming” in general is the treatment of the software itself as a meaningful artifact and product. Many people program: artists, grad students, scientists, data people, Excel jockeys, people in game jams… But they rarely care about version control, backwards compatibility, API usability, modularity, etc. They think of software as a step to getting something done, not something important itself.\nUnexpected! When looking at the legacy of Naur, of people late career coming to epiphanies that software’s less important than the mental models (documented through software)…\nI think here are 2 mental models: of the problem and of the software solving the problem. Software engineering builds the second model too. Software engineering’s when you can’t just tell the LLM to “write a program to do X” and be satisfied, because you have to consider constraints and scope beyond the individual problem, beyond the functional requirements. Performance, security, auditability, privacy, compliance, even compatibility with your current tech stack, considering them makes it engineering.\nHow do you collect requirements adequately? You discussed the spec-&gt;implementation problem, what about the BA problem of mysterious-world-&gt;spec?\nThe answer is surprisingly boring: spend a lot of time speccing it with a domain expert. FM helps an engineer do their job better, but it doesn’t help an engineer who doesn’t want to do the job. Part of that job is figuring out what the problem to solve is.\nOur industry ignores correctness. Tell me why!\nJust the boring, cynical answer: “bugs don’t cost enough to be worth squashing early”. The companies most receptive to FM expect really expensive bugs to be both possible and subtle. Many companies aren’t interested because they think they can catch their bugs with other methods or think they don’t have expensive bugs at all. And if they’re wrong, they find out too late.\nHow technical does management need to be to consider these things? I find many businesses have poor oversight of internal operations and make poor assumptions when deciding opportunity costs, etc.\nI believe management should be quite technical to catch these things, but I don’t want to be too judgmental; I have very little insight into what large businesses look like from the top. Maybe I’d take one look at that and give up on FM forever. Past a few hundred employees and you can’t form a good mental model of all your employees.\n\n[1] The original phrasing was:\n\nOne common way of doing that: 1. write a spec. 2. generate the spec’s state space 3. take behaviors in that state space, sequences of steps, and turn that into a sequence of commands on the program. When fed the same commands, the program should end up in the same states.\n",
    "full_content": "Lobsters Interview with Hwayne\n\nFor this interview, I spoke with hwayne a few times.\n\nLast week, we talked to icefox, next week we’ll hear from matklad. Thank you to my proof readers! <3\n\nYo! To prove it’s me, here’s a hash of our last PM: a4332cf8fb02ff3f02de333343958021 . Not for security, but to signal my values!\n\nMuch of my childhood online was collecting common wisdom, memes, fuddlore and repeating them to signal my virtue. Growing has been the long path of killing those darlings. Anyway, I love your work!\n\nLet’s see how fast I can disillusion you!\n\nIntroduce yourself! How’d you discover programming and Formal Methods (FM)?\n\nI’m Hillel. I’m mostly known online for writing about software, especially FM (to make it more accessible), and history. In my free time I juggle and make chocolate.\n\nI always liked computers and tried to learn C++ to make video games as a teen: not an experience I’d recommend to people. (Actually, I learned Drape and Visual Basic before that. I also miss VB6, it had nice features). In college I did Physics and Math with the hope of becoming a physicist, but got scared off by grad school, because all of my friends who graduated before me, seemed miserable. Long hours, low pay, bad social life, and for five years…\n\nBesides, universities and industrial labs push the boundaries of human knowledge, but I really like “pop research”: aggregating existing knowledge and presenting it in engaging or entertaining form. My dives into exotic languages are a good example, like J and Picat. Indeed, FM is a manifestation of my real passion of falling down rabbit holes researching. Besides, community puts me in touch with lots of academics and exotic tool users, who love to share their weird stuff with me, like MiniZinc and constraint solving!\n\nAnyway, I pivoted into programming because at the time (2013) that was the easiest way for educated people to switch career paths. And all the money was in webdev, so I became a webdev. Later at espark, which managed iPad apps for elementary schools, our system ended up being “accidentally distributed”: interactions between us, Apple APIs, school infrastructure, and their mobile device managers led to lots of issues you see in distributed systems, despite us all doing Rails and JavaScript. At this point, two things happened:\n\nI researched Idris, which led me to internet arguments about FM, which in turn led me to reading the Amazon TLA+ paper “Use of FM at AWS”. I was officially diagnosed with ADHD and put on Ritalin.\n\nImportant question for the interviewer: have you ever taken ADHD meds?\n\nI have not! Basically, for the first couple of days after you start meds, you are a GOD. Nothing can stop you. You can do anything. After a few days, your body adjusts and balances out, so you get the ability to focus without the mania. For my first two days, I crammed TLA+. This head start really helped get over the hump. I immediately caught some major bugs in the redesign, which made me fall in love. Cue years of trying to make it more accessible for people without the Ritalin God-mode head start!\n\nBut evangelism is difficult. At espark, nobody else wanted to learn TLA+ because I already knew it. The vibe was that it solved a one-off problem and they wouldn’t need it again. After I left, about once a quarter they would tell me about a new hairy bug that would have been caught by TLA+, always “a one-off problem that won’t happen again”.\n\nI see this as a manifestation of just how high the cost is for these tools. If it took five minutes to learn and apply, they’d do it. But they expect weeks of practice to study and solve problems and think “ugh no”.\n\nIn your evangelism, you talk of the business case sometimes. How do you balance that for a company vs. the personal effort/career hurdles of developers?\n\nI often ask myself: Does spending ten hours learning FM or bash scripting really well help the average engineer more? Probably bash. Right now, FM is a very bottom-up practice where developers see it, get inspired, and apply it at their work. Very rarely is it top-down, where a VP or director mandates it. The researchers and evangelists —including me— are devs. The TLA+ foundation is trying to figure out how to make that jump though. We have anecdotal claims of FM catching expensive bugs and saving money, but it’s hard to put numbers or measure the cost/reward, which makes decision-makers shy away.\n\nThere’s then the perennial challenge of “okay we’ve modeled the specification, how do we actually make sure our implementation matches this?”\n\nThe Go team wrote 2 implementations, to keep the spec honest.\n\nPart of the value of a formal specification (spec) is that it can be so abstract. Specs omit many details a generator must make. But there’s vast potential in using specs to test code. MongoDB has done a lot and I’ve made “test generators” for clients (which I’ll blog about.) I’m generally a fan of test driven development, because I know if I don’t write them ahead of time, I",
    "source": "lobste.rs",
    "group": "community_signals",
    "date": "2025-08-08 21:35",
    "_score": 6
  },
  {
    "title": "Genie 3 as a realism filter connected to an LLM: is it AGI or SSI?",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1mllomc/genie_3_as_a_realism_filter_connected_to_an_llm/",
    "content": "Let's say an LLM creates a prompt for Genie 3. Genie 3 creates a spatio-temporally and causally real world simulation. Let's say we create objects and quantities in Genie 3. Or we create letters in a spatial form. A multimodal LLM can analyze this modelled world and the objects in it - all created by Genie 3 - and create a new reply prompt. The &quot;realism&quot; engine of Genie 3 can filter out unreal or hallucinated facts about reality this way. This can continue forever in a loop. So is AGI or SSI very near? Genie 3 uses a step-by-step process to create each frame of the world based on what happened before, making sure things like gravity and collisions happen naturally. It is trained on tons of real videos, so it learns how the world actually works, like how water flows or leaves move in the wind, without needing specific rules programmed in. You can add or change things on the fly with prompts, and it adjusts the world smoothly while keeping physics realistic.  It has a built-in memory that remembers the world for up to a minute, so changes stick and nothing randomly appears or disappears. The stability comes naturally from its training, avoiding weird inconsistencies without forcing strict 3D rules. Genie 3 works in a smart compressed space that ties user actions to the world's state, preventing drift or random errors over time.  &#32; submitted by &#32;  /u/custodiam99   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nLet's say an LLM creates a prompt for Genie 3. Genie 3 creates a spatio-temporally and causally real world simulation. Let's say we create objects and quantities in Genie 3. Or we create letters in a spatial form. A multimodal LLM can analyze this modelled world and the objects in it - all created by Genie 3 - and create a new reply prompt. The \"realism\" engine of Genie 3 can filter out unreal or hallucinated facts about reality this way. This can continue forever in a loop. So is AGI or SSI very near?\n\nGenie 3 uses a step-by-step process to create each frame of the world based on what happened before, making sure things like gravity and collisions happen naturally.\n\nIt is trained on tons of real videos, so it learns how the world actually works, like how water flows or leaves move in the wind, without needing specific rules programmed in.\n\nYou can add or change things on the fly with prompts, and it adjusts the world smoothly while keeping physics realistic.\n\nIt has a built-in memory that remembers the world for up to a minute, so changes stick and nothing randomly appears or disappears.\n\nThe stability comes naturally from its training, avoiding weird inconsistencies without forcing strict 3D rules.\n\nGenie 3 works in a smart compressed space that ties user actions to the world's state, preventing drift or random errors over time.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 09:55",
    "_score": 6
  },
  {
    "title": "[Model] Gemma3n MM (#20495)",
    "link": "https://github.com/vllm-project/vllm/commit/5a16fa614c78e1f401125cd7384c602f83cb2160",
    "content": "[Model] Gemma3n MM (#20495)\n\nSigned-off-by: ShriKode &lt;shrikode@gmail.com&gt;\nSigned-off-by: NickLucche &lt;nlucches@redhat.com&gt;\nSigned-off-by: Roger Wang &lt;hey@rogerw.me&gt;\nCo-authored-by: ShriKode &lt;shrikode@gmail.com&gt;\nCo-authored-by: Roger Wang &lt;hey@rogerw.me&gt;",
    "full_content": "[Model] Gemma3n MM (#20495) · vllm-project/vllm@5a16fa6\n\n+\n\n- There's no PLE caching or out-of-memory swapping support, as described in [Google's blog](https://developers.googleblog.com/en/introducing-gemma-3n/). These features might be too model-specific for vLLM, and swapping in particular may be better suited for constrained setups.",
    "source": "github.com",
    "group": "community_signals",
    "date": "2025-08-09 16:56",
    "_score": 5
  },
  {
    "title": "Implicit language-model-only mode via limit-mm-per-prompt (#22299)",
    "link": "https://github.com/vllm-project/vllm/commit/08b751ba749541259e5450d6371d822fdf769b8a",
    "content": "Implicit language-model-only mode via limit-mm-per-prompt (#22299)\n\nSigned-off-by: Roger Wang &lt;hey@rogerw.me&gt;\nSigned-off-by: Andy Xie &lt;andy.xning@gmail.com&gt;\nSigned-off-by: tjtanaa &lt;tunjian.tan@embeddedllm.com&gt;\nSigned-off-by: Andrew Sansom &lt;andrew@protopia.ai&gt;\nSigned-off-by: Zhiyu Cheng &lt;zhiyuc@nvidia.com&gt;\nSigned-off-by: Shu Wang &lt;shuw@nvidia.com&gt;\nSigned-off-by: Po-Han Huang &lt;pohanh@nvidia.com&gt;\nSigned-off-by: Shu Wang. &lt;shuw@nvidia.com&gt;\nSigned-off-by: XIn Li &lt;xinli@nvidia.com&gt;\nSigned-off-by: Junhao Li &lt;junhao@ubicloud.com&gt;\nSigned-off-by: chaunceyjiang &lt;chaunceyjiang@gmail.com&gt;\nSigned-off-by: zRzRzRzRzRzRzR &lt;2448370773@qq.com&gt;\nSigned-off-by: zitian.zhao &lt;zitian.zhao@tencentmusic.com&gt;\nSigned-off-by: zitian zhao &lt;zitian.zhao@tencentmusic.com&gt;\nSigned-off-by: DarkLight1337 &lt;tlleungac@connect.ust.hk&gt;\nSigned-off-by: iAmir97 &lt;Amir.balwel@embeddedllm.com&gt;\nSigned-off-by: iAmir97 &lt;71513472+iAmir97@users.noreply.github.com&gt;\nSigned-off-by: Linkun &lt;github@lkchen.net&gt;\nCo-authored-by: Ning Xie &lt;andy.xning@gmail.com&gt;\nCo-authored-by: TJian &lt;tunjian.tan@embeddedllm.com&gt;\nCo-authored-by: Andrew Sansom &lt;andrew@protopia.ai&gt;\nCo-authored-by: Zhiyu &lt;zhiyuc@nvidia.com&gt;\nCo-authored-by: Shu Wang &lt;shuw@nvidia.com&gt;\nCo-authored-by: XIn Li &lt;xinli@nvidia.com&gt;\nCo-authored-by: Junhao Li &lt;streaver91@gmail.com&gt;\nCo-authored-by: Chauncey &lt;chaunceyjiang@gmail.com&gt;\nCo-authored-by: Yuxuan Zhang &lt;2448370773@qq.com&gt;\nCo-authored-by: ZiTian Zhao &lt;zitian.zhao@tencentmusic.com&gt;\nCo-authored-by: Cyrus Leung &lt;tlleungac@connect.ust.hk&gt;\nCo-authored-by: Po-Han Huang (NVIDIA) &lt;53919306+nvpohanh@users.noreply.github.com&gt;\nCo-authored-by: iAmir97 &lt;71513472+iAmir97@users.noreply.github.com&gt;\nCo-authored-by: iAmir97 &lt;Amir.balwel@embeddedllm.com&gt;\nCo-authored-by: gemini-code-assist[bot] &lt;176961590+gemini-code-assist[bot]@users.noreply.github.com&gt;\nCo-authored-by: Cyrus Leung &lt;cyrus.tl.leung@gmail.com&gt;\nCo-authored-by: Hong Hanh &lt;hanh.usth@gmail.com&gt;\nCo-authored-by: youkaichao &lt;youkaichao@gmail.com&gt;\nCo-authored-by: lkchen &lt;github@lkchen.net&gt;",
    "full_content": "Implicit language-model-only mode via limit-mm-per-prompt (#22299) · vllm-project/vllm@08b751b\n\nif multimodal_config . get_limit_per_prompt ( \"image\" ) or \\\n\nself . visual = None\n\n1358",
    "source": "github.com",
    "group": "community_signals",
    "date": "2025-08-09 05:21",
    "_score": 5
  },
  {
    "title": "Humans make better content cops than AI, but cost 40x more",
    "link": "https://www.theregister.com/2025/08/08/humans_outperform_ai_models_brand_safety/",
    "content": "Article URL: https://www.theregister.com/2025/08/08/humans_outperform_ai_models_brand_safety/\nComments URL: https://news.ycombinator.com/item?id=44847017\nPoints: 2\n# Comments: 1",
    "full_content": "Humans outperform AI models for brand safety, but cost more\n\nHuman content moderators still outperform AI when it comes to recognizing policy-violating material, but they also cost significantly more.\n\nMarketers looking to ensure that their ads do not surface in a toxic slurry face a dilemma – spend more money or see more Hitler.\n\nResearchers affiliated with AI brand protection biz Zefr did the math, detailed in a preprint paper titled \"AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety.\"\n\nThe paper, accepted at the upcoming Computer Vision in Advertising and Marketing (CVAM) workshop at the 2025 International Conference on Computer Vision, presents an analysis of the cost and effectiveness of multimodal large language models (MLLMs) for brand safety tasks.\n\nThe researchers' calculations show human moderation to be a premium indulgence, at almost 40x the most cost-efficient machine learning labor.\n\nBrand safety means preventing inappropriate content from becoming associated with a brand and damaging that brand's reputation. It has become something of a moving target in the wake of the Trump administration's rollback of diversity, equity, and inclusion. It's distinct from consumer-facing content moderation on social media sites like Meta's Instagram, which stands accused of knowingly distributing content that's harmful [PDF] and faces related litigation.\n\nThe Zefr team explains, \"Advertisers define content categories they wish to avoid; ranging from violent or adult-themed material to controversial political discourse. While general content moderation aims to identify and manage policy-violating content, brand safety is specifically concerned with aligning ad placements with advertiser preferences.\"\n\nTypically, the authors say, brand safety efforts involve a combination of human review and machine learning-based analysis of imagery, audio, and text. The purpose of the study was to look at whether MLLMs can do the job well and at what cost.\n\nThey evaluated six models – GPT-4o, GPT-4o-mini, Gemini-1.5-Flash, Gemini-2.0-Flash, Gemini-2.0-Flash-Lite, and Llama-3.2-11B-Vision – and human review using a dataset of 1500 videos, consisting of 500 videos from each of the following categories: Drugs, Alcohol and Tobacco (DAT); Death, Injury and Military Conflict (DIMC); and Kid’s Content.\n\nResearchers scored performance in each of three categories: precision, recall, and F1, which are common methods for machine learning evaluation. Precision refers to predicted positive classifications (policy violations) of content compared to actual positive instances in the dataset; recall refers to the percentage of actual positive instances classified correctly; and F1 is the harmonic mean of precision and recall.\n\nOverall scores (precision, recall, F1) were as follows, where 1.00 would represent 100 percent accuracy, with no false positives or false negatives:\n\nModel Precision Recall F1 GPT-4o 0.94 0.83 0.87 GPT-4o-mini 0.92 0.85 0.88 Gemini-1.5-Flash 0.86 0.96 0.90 Gemini-2.0-Flash 0.84 0.98 0.91 Gemini-2.0-Flash-Lite 0.87 0.95 0.91 Llama-3.2-11B-Vision 0.87 0.86 0.86 Human 0.98 0.97 0.98\n\n\"Among the MLLMs the Gemini models emerge as the best overall models, outperforming the others in terms of F1-score,\" the researchers state in their paper, adding that, interestingly, the compact versions of these models do not perform significantly worse.\n\n\"These results underscore the effectiveness of MLLMs in automating content moderation but also highlight the continued superiority of human reviewers in accuracy, particularly in more complex or nuanced classifications where context and deep understanding are required,\" the paper states.\n\nThe researchers also observed that these models often failed due to incorrect associations, a lack of contextual understanding, and language differences. One example they cited is a video discussing caffeine addiction in the Japanese language that was incorrectly flagged as a drug category violation by all the models. The authors attributed this to flawed associations with the term addiction and gaps in the contextual understanding of Japanese. Generally, they said that these models exhibit poorer performance for non-English content.\n\nIn terms of cost, superior human moderation looks like a luxury. Here's how the models compare in terms of F1 score and price.\n\nModel F1 Cost GPT-4o 0.87 $419 GPT-4o-mini 0.88 $25 Gemini-1.5-Flash 0.90 $28 Gemini-2.0-Flash 0.91 $56 Gemini-2.0-Flash-Lite 0.91 $28 Llama-3.2-11B-Vision 0.86 $459 Human 0.98 $974\n\n\"We showed that the compact MLLMs offer a significantly cheaper alternative compared to their larger counterparts without sacrificing accuracy,\" the authors conclude. \"However, human reviewers remain superior in accuracy, particularly in complex or nuanced classifications.\"\n\n\"While multimodal large language models like Gemini and GPT can handle brand safety video moderation across text, audio and visuals with surp",
    "source": "theregister.com",
    "group": "community_signals",
    "date": "2025-08-09 14:58",
    "_score": 5
  },
  {
    "title": "OpenAI’s Doomsday Prepper CEO Sam Altman Stockpiles ‘Guns, Gold, Potassium Iodide, Antibiotics, Batteries, Water, and Gas Masks’",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1mlhaip/openais_doomsday_prepper_ceo_sam_altman/",
    "content": "OpenAI’s Doomsday Prepper CEO Sam Altman Stockpiles ‘Guns, Gold, Potassium Iodide, Antibiotics, Batteries, Water, and Gas Masks’ July 31, 2025 Sam Altman, a central figure in the advancing world of artificial intelligence, is candid about his personal emergency supplies and contingency plans. Describing his approach with, “I have guns, gold, potassium iodide, antibiotics, batteries, water, gas masks from the Israeli Defense Force, and a big patch of land in Big Sur I can fly to,” Altman encapsulates a perspective marked by preparedness and vigilance. Foundations of Foresight Altman’s journey began with an early passion for technology, teaching himself to code at a young age before launching successful startups and eventually taking the helm at OpenAI, one of the world’s leading AI research organizations. His leadership at OpenAI is defined by both rapid innovation and a pronounced focus on existential risk—traits that help explain his survivalist inclinations. Throughout his public career, Altman has repeatedly emphasized the unpredictable dangers accompanying modern progress, identifying threats such as engineered pandemics, artificial intelligence run amok, and geopolitical instability. His reference to stockpiling items like gold, water, antibiotics, and even military-grade gas masks demonstrates not only personal caution but also the growing culture of risk management within the tech elite. Context for Caution The rationale behind Altman’s preparations is rooted in recent history as well as his direct experience steering foundational AI developments. Influential moments—such as public health scares, breakthroughs in synthetic biology, and mounting debate over AI safety — have amplified concerns among leaders capable of influencing technology’s future trajectory. Altman’s choice of specific gear reveals an understanding of both biological and technological threat vectors. Potassium iodide is a prophylactic against radiation exposure in nuclear incidents. Gas masks and antibiotics indicate anticipation of airborne pathogens or chemical hazards. Moreover, the mention of a retreat in Big Sur highlights a belief that, in some scenarios, rapid escape and self-sufficiency are rational considerations. Sam Altman’s opinions wield influence well beyond his own survival planning. As CEO of OpenAI, he is tasked with guiding responsible innovation while advocating for policies to mitigate the downsides of transformative technologies. His frankness about doomsday preparations — and the practical steps he takes — signal that even those at the epicenter of progress perceive risk in very concrete terms.  &#32; submitted by &#32;  /u/No-Author-2358   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nOpenAI’s Doomsday Prepper CEO Sam Altman Stockpiles ‘Guns, Gold, Potassium Iodide, Antibiotics, Batteries, Water, and Gas Masks’\n\nJuly 31, 2025\n\nSam Altman, a central figure in the advancing world of artificial intelligence, is candid about his personal emergency supplies and contingency plans. Describing his approach with, “I have guns, gold, potassium iodide, antibiotics, batteries, water, gas masks from the Israeli Defense Force, and a big patch of land in Big Sur I can fly to,” Altman encapsulates a perspective marked by preparedness and vigilance.\n\nFoundations of Foresight\n\nAltman’s journey began with an early passion for technology, teaching himself to code at a young age before launching successful startups and eventually taking the helm at OpenAI, one of the world’s leading AI research organizations. His leadership at OpenAI is defined by both rapid innovation and a pronounced focus on existential risk—traits that help explain his survivalist inclinations.\n\nThroughout his public career, Altman has repeatedly emphasized the unpredictable dangers accompanying modern progress, identifying threats such as engineered pandemics, artificial intelligence run amok, and geopolitical instability. His reference to stockpiling items like gold, water, antibiotics, and even military-grade gas masks demonstrates not only personal caution but also the growing culture of risk management within the tech elite.\n\nContext for Caution\n\nThe rationale behind Altman’s preparations is rooted in recent history as well as his direct experience steering foundational AI developments. Influential moments—such as public health scares, breakthroughs in synthetic biology, and mounting debate over AI safety — have amplified concerns among leaders capable of influencing technology’s future trajectory.\n\nAltman’s choice of specific gear reveals an understanding of both biological and technological threat vectors. Potassium iodide is a prophylactic against radiation exposure in nuclear incidents. Gas masks and antibiotics indicate anticipation of airborne pathogens or chemical hazards. Moreover, the mention of a retreat in Big Sur highlights a belief that, in some scenarios, rapid escape and self-sufficiency are rational considerations.\n\nSam Altman’s opinions wield influence well beyond his own survival planning. As CEO of OpenAI, he is tasked with guiding responsible innovation while advocating for policies to mitigate the downsides of transformative technologies. His frankness about doomsday preparations — and the practical steps he takes — signal that even those at the epicenter of progress perceive risk in very concrete terms.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 05:11",
    "_score": 5
  },
  {
    "title": "Anyone else finding it tricky to generate realistic human figures with current AI image generators without triggering their filters?",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1mln2n6/anyone_else_finding_it_tricky_to_generate/",
    "content": "Lately, I've been diving deeper into using AI image generators for creating realistic Images of AI Models that I can use for Social Media and Marketing, and I've noticed challenges and restrictions that I'm curious if others are experiencing. I've been playing around with generators like Midjourney, Stable Diffusion, and Leonardo AI, and while they are incredibly powerful for many things, generating consistent and accurate human figures across sessions is very difficult. For example, I've noticed certain words or contexts seem to trigger filters or just lead to nonsensical results. It's almost like the AI has a hard time interpreting certain everyday scenarios involving people. I even tried to generate an image related to sleep and found that the word &quot;bed&quot; in my prompt seemed to throw things off completely, leading to bizarre or filtered outputs saying it's explicit. Beyond specific word triggers, I've also found Inconsistency in Anatomy with some features sometimes coming out distorted. While I understand the need for safety measures, sometimes the restrictions feel a bit too broad and can limit creative exploration in non-harmful ways. It feels like while these tools are rapidly evolving, generating realistic depictions of humans in various situations still has a long way to go. Has anyone else run into similar issues or frustrating limitations when trying to generate images of people what have your experiences been like with specific keywords or scenarios and have you found any prompts or techniques that help overcome these would love to hear your thoughts and see if this is a common experience!  &#32; submitted by &#32;  /u/Blitzgert   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nLately, I've been diving deeper into using AI image generators for creating realistic Images of AI Models that I can use for Social Media and Marketing, and I've noticed challenges and restrictions that I'm curious if others are experiencing. I've been playing around with generators like Midjourney, Stable Diffusion, and Leonardo AI, and while they are incredibly powerful for many things, generating consistent and accurate human figures across sessions is very difficult. For example, I've noticed certain words or contexts seem to trigger filters or just lead to nonsensical results. It's almost like the AI has a hard time interpreting certain everyday scenarios involving people. I even tried to generate an image related to sleep and found that the word \"bed\" in my prompt seemed to throw things off completely, leading to bizarre or filtered outputs saying it's explicit. Beyond specific word triggers, I've also found Inconsistency in Anatomy with some features sometimes coming out distorted. While I understand the need for safety measures, sometimes the restrictions feel a bit too broad and can limit creative exploration in non-harmful ways. It feels like while these tools are rapidly evolving, generating realistic depictions of humans in various situations still has a long way to go. Has anyone else run into similar issues or frustrating limitations when trying to generate images of people what have your experiences been like with specific keywords or scenarios and have you found any prompts or techniques that help overcome these would love to hear your thoughts and see if this is a common experience!",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 11:23",
    "_score": 5
  },
  {
    "title": "AI model uses audio to help protect endangered species",
    "link": "https://blog.google/technology/google-deepmind/perch-ai-model/",
    "content": "Article URL: https://blog.google/technology/google-deepmind/perch-ai-model/\nComments URL: https://news.ycombinator.com/item?id=44848942\nPoints: 1\n# Comments: 0",
    "full_content": "New Perch AI model helps protect endangered species\n\nGoogle DeepMind released a new version of their AI model Perch, which helps conservationists analyze bioacoustic data from diverse ecosystems This update is generalized to a wider range of animals, so it can analyze audio from Hawaiian honeycreepers to coral reefs.\n\n\n\nScientists use microphones and underwater hydrophones to collect audio dense with animal vocalizations, but making sense of so much data is a massive undertaking. This new version of Perch, which is an open model available on Kaggle, is helping scientists protect endangered species by helping them process that data, so scientists can focus their efforts on on-the-ground work.\n\n\n\nLearn more about Perch on the Google DeepMind blog.",
    "source": "blog.google",
    "group": "community_signals",
    "date": "2025-08-09 18:35",
    "_score": 4
  },
  {
    "title": "Why Boring Businesses Outlast AI Hype Cycles",
    "link": "https://news.ycombinator.com/item?id=44848018",
    "content": "Everyones building an AI company these days Every pitch deck leads with AIpowered every startup claims to be the next ChatGPT for X and venture capitalists cant stop talking about the artificial intelligence revolution\nBut heres what theyre missing while everyone crowds into the same overhyped pond the real money is being made in boring businesses that solve unglamorous problems\nThe Government Forms Gold Mine\nLet me tell you about a company that perfectly illustrates this principle While thousands of entrepreneurs were building GPT wrappers and AI chatbots a small team noticed something everyone else ignored people applying for government assistance were drowning in paperwork\nThe process was bureaucratic hell—confusing forms, unclear requirements, and endless back-and-forth with government offices. Most tech entrepreneurs wouldn't give this problem a second glance. It's not sexy. It won't make TechCrunch headlines. It doesn't involve machine learning or neural networks.\nBut guess what? That \"boring\" form-filling software is now generating $30 million in annual revenue.\nThis is the essence of \"fishing where the fish are, not where the fishermen are.\" While everyone else battles over the same crowded markets, smart entrepreneurs find untapped problems with real paying customers.\nWhy Boring Usually Beats Buzzy\nThe AI gold rush reminds me of every other tech bubble. Remember when everything was \"blockchain-powered\" or \"mobile-first\" or \"social-enabled\"? The companies that survived those hype cycles weren't the ones chasing trends—they were the ones solving real problems that happened to use the technology.\nToday's AI companies face three fundamental problems:\nThe Commodity Trap: As AI capabilities become more accessible, competitive moats disappear. Your AI chatbot for lawyers looks remarkably similar to everyone else's AI chatbot for lawyers.\nThe Hype Hangover: When the AI bubble deflates (and it will), investors will demand actual profits, not just impressive demos. Many current AI companies have no clear path to profitability.\nThe Dependency Risk: Building your entire business on rapidly evolving AI models means you're one API change away from obsolescence.\nHow to Find Your Boring Gold Mine\nFinding these overlooked opportunities isn't luck—it's a systematic approach to problem identification:\nStart With Your Own Pain Points\nThe best business ideas often hide in your daily frustrations. What processes make you want to scream? What tasks eat up hours of your time for no good reason? If it annoys you, it probably annoys thousands of other people too.\nMine Your Network\nYour friends, family, and colleagues are goldmines of business ideas. Ask them: \"What's the most frustrating part of your job?\" \"What takes way longer than it should?\" \"What would save you hours every week?\" Listen for patterns—if multiple people mention similar problems, you've found something worth exploring.\nQuestion Everything Manual\nIn 2025, if someone's still using spreadsheets to track important business processes, there's probably an opportunity. If people are printing forms, filling them out by hand, and faxing them back, there's definitely an opportunity. Look for workflows that seem stuck in 1995.\nFollow the Complaints\nReddit, Twitter, and industry forums are filled with people complaining about broken processes. These complaint threads are basically free market research.\nWhile flashy startups fight for attention, boring businesses enjoy several unfair advantages:\nLess Competition: Most entrepreneurs chase shiny objects, leaving mundane problems undersolved.\nSticky Customers: Businesses that solve operational headaches create deep integration points that are hard to replace\nPredictable Revenue: Boring problems tend to be ongoing problems, leading to subscription revenue rather than one-time purchases\nLower Customer Acquisition Costs When you solve a real pain point in an underserved market customers find you through word-of-mouth\n\nComments URL: https://news.ycombinator.com/item?id=44848018\nPoints: 2\n# Comments: 0",
    "full_content": "Why Boring Businesses Outlast AI Hype Cycles\n\nEveryones building an AI company these days Every pitch deck leads with AIpowered every startup claims to be the next ChatGPT for X and venture capitalists cant stop talking about the artificial intelligence revolution But heres what theyre missing while everyone crowds into the same overhyped pond the real money is being made in boring businesses that solve unglamorous problems The Government Forms Gold Mine Let me tell you about a company that perfectly illustrates this principle While thousands of entrepreneurs were building GPT wrappers and AI chatbots a small team noticed something everyone else ignored people applying for government assistance were drowning in paperwork The process was bureaucratic hell—confusing forms, unclear requirements, and endless back-and-forth with government offices. Most tech entrepreneurs wouldn't give this problem a second glance. It's not sexy. It won't make TechCrunch headlines. It doesn't involve machine learning or neural networks. But guess what? That \"boring\" form-filling software is now generating $30 million in annual revenue. This is the essence of \"fishing where the fish are, not where the fishermen are.\" While everyone else battles over the same crowded markets, smart entrepreneurs find untapped problems with real paying customers. Why Boring Usually Beats Buzzy The AI gold rush reminds me of every other tech bubble. Remember when everything was \"blockchain-powered\" or \"mobile-first\" or \"social-enabled\"? The companies that survived those hype cycles weren't the ones chasing trends—they were the ones solving real problems that happened to use the technology. Today's AI companies face three fundamental problems: The Commodity Trap: As AI capabilities become more accessible, competitive moats disappear. Your AI chatbot for lawyers looks remarkably similar to everyone else's AI chatbot for lawyers. The Hype Hangover: When the AI bubble deflates (and it will), investors will demand actual profits, not just impressive demos. Many current AI companies have no clear path to profitability. The Dependency Risk: Building your entire business on rapidly evolving AI models means you're one API change away from obsolescence. How to Find Your Boring Gold Mine Finding these overlooked opportunities isn't luck—it's a systematic approach to problem identification: Start With Your Own Pain Points The best business ideas often hide in your daily frustrations. What processes make you want to scream? What tasks eat up hours of your time for no good reason? If it annoys you, it probably annoys thousands of other people too. Mine Your Network Your friends, family, and colleagues are goldmines of business ideas. Ask them: \"What's the most frustrating part of your job?\" \"What takes way longer than it should?\" \"What would save you hours every week?\" Listen for patterns—if multiple people mention similar problems, you've found something worth exploring. Question Everything Manual In 2025, if someone's still using spreadsheets to track important business processes, there's probably an opportunity. If people are printing forms, filling them out by hand, and faxing them back, there's definitely an opportunity. Look for workflows that seem stuck in 1995. Follow the Complaints Reddit, Twitter, and industry forums are filled with people complaining about broken processes. These complaint threads are basically free market research. While flashy startups fight for attention, boring businesses enjoy several unfair advantages: Less Competition: Most entrepreneurs chase shiny objects, leaving mundane problems undersolved. Sticky Customers: Businesses that solve operational headaches create deep integration points that are hard to replace Predictable Revenue: Boring problems tend to be ongoing problems, leading to subscription revenue rather than one-time purchases Lower Customer Acquisition Costs When you solve a real pain point in an underserved market customers find you through word-of-mouth",
    "source": "news.ycombinator.com",
    "group": "community_signals",
    "date": "2025-08-09 16:49",
    "_score": 4
  },
  {
    "title": "Beeve.ai – Your AI agent Built for conversion",
    "link": "https://beeve.ai/en",
    "content": "Article URL: https://beeve.ai/en\nComments URL: https://news.ycombinator.com/item?id=44847941\nPoints: 1\n# Comments: 1",
    "full_content": "Beeve – Your AI chatbot for smarter customer service 🐝\n\nGet a free demo with your own content\n\nReady to test your own AI-chatbot?\n\nWe are a small team that is passionate about making your customer service smarter with AI. Sign up – and we'll send you a demo built on your own content and help you get started.",
    "source": "beeve.ai",
    "group": "community_signals",
    "date": "2025-08-09 16:41",
    "_score": 4
  },
  {
    "title": "Qwen 3 0.6B beats GPT-5 in simple math",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlsm8e/qwen_3_06b_beats_gpt5_in_simple_math/",
    "content": "      I saw this comparison between Grok and GPT-5 on X for solving the equation 5.9 = x + 5.11. In the comparison, Grok solved it but GPT-5 without thinking failed. It could have been handpicked after multiples runs, so out of curiosity and for fun I decided to test it myself. Not with Grok but with local models running on iPhone since I develop an app around that, Locally AI for those interested but you can reproduce the result below with LMStudio, Ollama or any other local chat app of course. And I was honestly surprised.In my very first run, GPT-5 failed (screenshot) while Qwen 3 0.6B without thinking succeeded. After multiple runs, I would say GPT-5 fails around 30-40% of the time, while Qwen 3 0.6B, which is a tiny 0.6 billion parameters local model around 500 MB in size, solves it every time.Yes it’s one example, GPT-5 was without thinking and it’s not really optimized for math in this mode but Qwen 3 too. And honestly, it’s a simple equation I did not think GPT-5 would fail to solve, thinking or not. Of course, GPT-5 is better than Qwen 3 0.6B, but it’s still interesting to see cases like this one.  &#32; submitted by &#32;  /u/adrgrondin   [link] &#32; [comments] ",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • adrgrondin Qwen 3 0.6B beats GPT-5 in simple math Generation I saw this comparison between Grok and GPT-5 on X for solving the equation 5.9 = x + 5.11. In the comparison, Grok solved it but GPT-5 without thinking failed. It could have been handpicked after multiples runs, so out of curiosity and for fun I decided to test it myself. Not with Grok but with local models running on iPhone since I develop an app around that, Locally AI for those interested but you can reproduce the result below with LMStudio , Ollama or any other local chat app of course. And I was honestly surprised. In my very first run, GPT-5 failed (screenshot) while Qwen 3 0.6B without thinking succeeded. After multiple runs, I would say GPT-5 fails around 30-40% of the time, while Qwen 3 0.6B, which is a tiny 0.6 billion parameters local model around 500 MB in size, solves it every time. Yes it’s one example, GPT-5 was without thinking and it’s not really optimized for math in this mode but Qwen 3 too. And honestly, it’s a simple equation I did not think GPT-5 would fail to solve, thinking or not. Of course, GPT-5 is better than Qwen 3 0.6B, but it’s still interesting to see cases like this one. Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 15:46",
    "_score": 4
  },
  {
    "title": "New GLM-4.5 models soon",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mljip4/new_glm45_models_soon/",
    "content": "      I hope we get to see smaller models. The current models are amazing but quite too big for a lot of people. But looks like teaser image implies vision capabilities. Image posted by Z.ai on X.  &#32; submitted by &#32;  /u/adrgrondin   [link] &#32; [comments] ",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • adrgrondin New GLM-4.5 models soon News I hope we get to see smaller models. The current models are amazing but quite too big for a lot of people. But looks like teaser image implies vision capabilities. Image posted by Z.ai on X. Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 07:28",
    "_score": 4
  },
  {
    "title": "Finally, I Wrote a 600-Page Book About My Mad LLM fine-tuning experiments",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlhryw/finally_i_wrote_a_600page_book_about_my_mad_llm/",
    "content": "You may or may not be aware that I wrote Training Pro and Playground and Virtual Lora and a lot of other insane code that some of you use every day to muck about with LLMs or to idly goof off. And not only that, but I have also created, in my own pathetic home, thousands and thousands of LoRAs and all kinds of strange, mutant models, some of which are actually pretty ok. I have been wanting to write this for some time, but have been saving it until I had some time on my hands, which is what I am doing right now: My last few years of feverish, frustrating, and occasionally glorious LLM experiments have been distilled into a real, live, actual book! I sort of got carried away, as always, and it would be close to 600 pages if printed in a big format. This is because, you know, once I get started, I cannot be stopped. It is a gigantic compendium of my own personal notes, ideas, lessons learned and tons of epic failures which I proudly present as shining examples of how not to do things. And I put in every of my secret tip and trick that I could think of. I even reproduced some of my old experiments, like Sydney, step by step, or the Plot Bot (even down to code on github to acquire and augment dataset), or the totally insane Style Transfer thing where I cruelly taunt Jane Austen mercilessly. (You can tell by the cowardly qualifier &quot;totally,&quot; that I am still kind of hung up about doing that.) But everything in there is real, I swear it, and I ran my computer around the clock, 24/7, to make sure that I could reproduce it all not just spew BS. It starts with a very pre-chewed &quot;bathroom theory&quot; of LLMs for super-newbs, (absolutely no math or highfalutin intellectual mumbo jumbo), and ends with how to gracefully handle all the delightful error messages and segfaults that are an integral part of the LLM fine-tuning experience. I don't know how it will be received, but this book contains Everything. I. Know. So I put the damned thing up on Amazon, apple, kobo..., and I don't expect it to make me famous or rich or anything, but if you would just look it up, and maybe even taking a cursory peek at a few pages, I would be, like, soooooo grateful. And while you are at it, you could, you know, buy it, and then write a raving review about how it made you instantly wise and enlightened, and how it opened your mind to the profound beauty and mystery of the universe and everything in it... and stuff. The book is titled, appropriately: The Cranky Man's Guide to LoRA &amp; QLoRA: Personal Lessons from a Thousand LLM Fine-Tuning Fails by F.P. Ham And he has a nice picture of a burning GPU on the cover, which I lovingly toiled over all weekend! It's also on apple book, B&amp;N and so on.  &#32; submitted by &#32;  /u/FPham   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • FPham Finally, I Wrote a 600-Page Book About My Mad LLM fine-tuning experiments Resources You may or may not be aware that I wrote Training Pro and Playground and Virtual Lora and a lot of other insane code that some of you use every day to muck about with LLMs or to idly goof off. And not only that, but I have also created, in my own pathetic home, thousands and thousands of LoRAs and all kinds of strange, mutant models, some of which are actually pretty ok. I have been wanting to write this for some time, but have been saving it until I had some time on my hands, which is what I am doing right now: My last few years of feverish, frustrating, and occasionally glorious LLM experiments have been distilled into a real, live, actual book! I sort of got carried away, as always, and it would be close to 600 pages if printed in a big format. This is because, you know, once I get started, I cannot be stopped. It is a gigantic compendium of my own personal notes, ideas, lessons learned and tons of epic failures which I proudly present as shining examples of how not to do things. And I put in every of my secret tip and trick that I could think of. I even reproduced some of my old experiments, like Sydney, step by step, or the Plot Bot (even down to code on github to acquire and augment dataset), or the totally insane Style Transfer thing where I cruelly taunt Jane Austen mercilessly. (You can tell by the cowardly qualifier \"totally,\" that I am still kind of hung up about doing that.) But everything in there is real, I swear it, and I ran my computer around the clock, 24/7, to make sure that I could reproduce it all not just spew BS. It starts with a very pre-chewed \"bathroom theory\" of LLMs for super-newbs, (absolutely no math or highfalutin intellectual mumbo jumbo), and ends with how to gracefully handle all the delightful error messages and segfaults that are an integral part of the LLM fine-tuning experience. I don't know how it will be received, but this book contains Everything. I. Know. So I put the damned thing up on Amazon, apple, kobo..., and I don't expect it to make me famous or rich or anything, but if you would just look it up, and maybe even taking a cursory peek at a few pages, I would be, like, soooooo grateful. And while you are at it, you could, you know, buy it, and then write a raving review about how it made you instantly wise and enlightened, and how it opened your mind to the profound beauty and mystery of the universe and everything in it... and stuff. The book is titled, appropriately: The Cranky Man's Guide to LoRA & QLoRA: Personal Lessons from a Thousand LLM Fine-Tuning Fails by F.P. Ham And he has a nice picture of a burning GPU on the cover, which I lovingly toiled over all weekend! It's also on apple book, B&N and so on. Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 05:40",
    "_score": 4
  },
  {
    "title": "Why aren't people using small llms to train on their own local datasets?",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mltedd/why_arent_people_using_small_llms_to_train_on/",
    "content": "Now that there are so many good small base model llms available why aren't we seeing people train them on their own data. Their every day to day work or home data/files on local models? I mean general llms like chatgpt are all great but most people have data lying around for their specific context/work that the general llms don't know about. So why aren't people using the smaller llms to train on those and make use of it? I feel like too much focus has been on the use of the general models without enough on how smaller models can be tuned on people's own data. Almost like the old PC vs Mainframe. In image/video i can see a plethora of loras but hardly any for llms. Is it a lack of easy to use tools like comfyui/AUTOMATIC1111 etc?  &#32; submitted by &#32;  /u/QFGTrialByFire   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • QFGTrialByFire Why aren't people using small llms to train on their own local datasets? Question | Help Now that there are so many good small base model llms available why aren't we seeing people train them on their own data. Their every day to day work or home data/files on local models? I mean general llms like chatgpt are all great but most people have data lying around for their specific context/work that the general llms don't know about. So why aren't people using the smaller llms to train on those and make use of it? I feel like too much focus has been on the use of the general models without enough on how smaller models can be tuned on people's own data. Almost like the old PC vs Mainframe. In image/video i can see a plethora of loras but hardly any for llms. Is it a lack of easy to use tools like comfyui/AUTOMATIC1111 etc? Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 16:18",
    "_score": 4
  },
  {
    "title": "AI max+ 395",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mlqylw/ai_max_395/",
    "content": "Anyone using a 128gb version with a local model as a serious replacement for commercial apis? If so, what device? What model? What tokens / second and context?  &#32; submitted by &#32;  /u/megadonkeyx   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • megadonkeyx AI max+ 395 Discussion Anyone using a 128gb version with a local model as a serious replacement for commercial apis? If so, what device? What model? What tokens / second and context? Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 14:36",
    "_score": 4
  },
  {
    "title": "uncensored gpt-oss-20b, bf16 and mxfp4 both available",
    "link": "https://www.reddit.com/r/LocalLLaMA/comments/1mli4za/uncensored_gptoss20b_bf16_and_mxfp4_both_available/",
    "content": "(please see comment for model download link, because reddit deletes my post if it contains link) gpt-oss-20b's refusal rate is super-high, ~70% on Amazon FalseReject dataset. I also tested it with a subset of WildChat 1M and saw about 5-10% refusal rate, which is almost untolerable. Unfortunately, current PTQ method hurts the LoRA adapter quite much (but sill better than nothing). We already get MXFP4 QAT working with gpt-oss and will keep everyone posted.  &#32; submitted by &#32;  /u/Ralph_mao   [link] &#32; [comments]",
    "full_content": "Go to LocalLLaMA r/LocalLLaMA r/LocalLLaMA https://x.com/localllamasub Subreddit to discuss AI & Llama, the large language model created by Meta AI. Members Online • Ralph_mao uncensored gpt-oss-20b, bf16 and mxfp4 both available Resources (please see comment for model download link, because reddit deletes my post if it contains link) gpt-oss-20b's refusal rate is super-high, ~70% on Amazon FalseReject dataset. I also tested it with a subset of WildChat 1M and saw about 5-10% refusal rate, which is almost untolerable. Unfortunately, current PTQ method hurts the LoRA adapter quite much (but sill better than nothing). We already get MXFP4 QAT working with gpt-oss and will keep everyone posted. Read more Share",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 06:02",
    "_score": 4
  },
  {
    "title": "OpenAI just priced GPT-5 so low it might trigger an AI price war ,who wins here?",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1ml44m6/openai_just_priced_gpt5_so_low_it_might_trigger/",
    "content": "OpenAI dropped GPT-5 this week, calling it “the best model in the world.” Whether you believe that or not, one thing is hard to ignore: the price is insanely low compared to the competition. Here’s the quick breakdown: GPT-5 API → $1.25 / 1M input tokens, $10 / 1M output tokens Google Gemini 2.5 Pro → Similar, but gets pricier with heavy usage Anthropic Claude Opus 4.1 → $15 / 1M input tokens, $75 / 1M output tokens Some devs are calling GPT-5’s pricing “a killer move” that could pressure Anthropic, Google, and others to slash their prices. If that happens, we could see the first real LLM price war something a lot of startups and indie devs have been waiting for. But here’s the catch: Big AI companies are spending tens of billions on infrastructure. Historically, that pushes costs up, not down. So is this just a temporary “shock price” to grab market share, or the start of long-term cheap AI? Question for you all: If prices really drop across the board, what do you think changes most the tools we build, the business models, or the type of people who can now afford to play in AI?  &#32; submitted by &#32;  /u/solo_trip-   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nOpenAI dropped GPT-5 this week, calling it “the best model in the world.” Whether you believe that or not, one thing is hard to ignore: the price is insanely low compared to the competition.\n\nHere’s the quick breakdown:\n\nGPT-5 API → $1.25 / 1M input tokens, $10 / 1M output tokens Google Gemini 2.5 Pro → Similar, but gets pricier with heavy usage Anthropic Claude Opus 4.1 → $15 / 1M input tokens, $75 / 1M output tokens Some devs are calling GPT-5’s pricing “a killer move” that could pressure Anthropic, Google, and others to slash their prices. If that happens, we could see the first real LLM price war something a lot of startups and indie devs have been waiting for. But here’s the catch: Big AI companies are spending tens of billions on infrastructure. Historically, that pushes costs up, not down. So is this just a temporary “shock price” to grab market share, or the start of long-term cheap AI?\n\nQuestion for you all: If prices really drop across the board, what do you think changes most the tools we build, the business models, or the type of people who can now afford to play in AI?",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-08 19:09",
    "_score": 4
  },
  {
    "title": "Backlash over \"horrible\" GPT-5 forces OpenAI to restore older ChatGPT models, double rate limit",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1mlwaze/backlash_over_horrible_gpt5_forces_openai_to/",
    "content": "GPT-5 is easily jailbroken, too  Facepalm: OpenAI boss Sam Altman said last month that GPT-5 was so fast and powerful that it actually scared him. The CEO compared it to having a &quot;superpower&quot; that offered &quot;legitimate PhD-level expert&quot; information on anything. But within a day of its launch, Altman has confirmed the older 4o models are being brought back as so many people dislike GPT-5: Article  &#32; submitted by &#32;  /u/gurugabrielpradipaka   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nWe value your privacy\n\nReddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 18:18",
    "_score": 4
  },
  {
    "title": "What happens when you treat GPTs as specialized employees instead of coding assistants.",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1mlw5h8/what_happens_when_you_treat_gpts_as_specialized/",
    "content": "A few months ago, I started a project to fix one of my own nagging problems which was a lack of focus, thanks to my phone. As a software engineer, my first instinct was to just start building, I set out on a coding spree, thinking I could solve it with brute force, just me and the code. This was the beginning of a journey that would completely change how I think about building software. Two months later the first version was ready. Instead of asking for feedback I just gave it to a friend and watched them use it and my illusions came crashing down. The flow that was so obvious to me was painfully difficult for him to understand. I realized the UX was just bad and my UI skills were way more amateur than I thought. I already had a custom GPT I used as a critical consultant for life decisions so I thought why not make a UX expert one. I made a detailed prompt for it and started feeding it my designs, mostly just screenshots and text descriptions of the flows. The feedback was a total bashing for my amateur thinking but it opened my eyes to so many new concepts. The key was I didn't just copy paste its advice. I actually pushed back and argued with it which forced me to learn the principles behind its decisions. Even with a better ux the app's core philosophy felt kinda incoherent. It just wasnt delivering the impact it should. So I went back to my critical consultant gpt. The prompt for this one is designed to be ruthless, no sugarcoating. I asked it to critique my app's core idea and as expected I got another bashing for how naive some of my assumptions were. This led to a third revamp from the ground up, but I was enjoying it because I was learning so much so fast. My whole workflow transformed. Creating specialized gpts, my ux guy, my strategy consultant, a junior coder for boilerplate, it felt less like I was a solo dev and more like a founder managing a team. It was a genuine moment of realization for me. I know it might sound obvious but I finally understood on a gut level why good founders obsess over hiring specialists. My job shifted completely. I was now spending 95% of my time on high level decisions, directing and interrogating the logic from my AI 'staff' instead of just coding. I can't overstate the accelerated learning I got from this. It's a different way of building things. Using AI not just to write code but to challenge your own thinking and expose your blind spots. there's a lot of doom and gloom around AI right now, but my experience with this project left me with a different feeling, for me the real power here isn't about replacing people. It's about giving a single person the leverage to do things that used to require a whole team. It felt less like I was being made obsolete, and more like I was being augmented. That feels like a silver lining worth talking about. Anyway, that's my story so far. The prompts themselves are nothing revolutionary, but if you want to see them or the final app for context, just let me know in the comments.  &#32; submitted by &#32;  /u/tabish_bshr   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nA few months ago, I started a project to fix one of my own nagging problems which was a lack of focus, thanks to my phone. As a software engineer, my first instinct was to just start building, I set out on a coding spree, thinking I could solve it with brute force, just me and the code. This was the beginning of a journey that would completely change how I think about building software.\n\nTwo months later the first version was ready. Instead of asking for feedback I just gave it to a friend and watched them use it and my illusions came crashing down. The flow that was so obvious to me was painfully difficult for him to understand. I realized the UX was just bad and my UI skills were way more amateur than I thought.\n\nI already had a custom GPT I used as a critical consultant for life decisions so I thought why not make a UX expert one. I made a detailed prompt for it and started feeding it my designs, mostly just screenshots and text descriptions of the flows. The feedback was a total bashing for my amateur thinking but it opened my eyes to so many new concepts. The key was I didn't just copy paste its advice. I actually pushed back and argued with it which forced me to learn the principles behind its decisions.\n\nEven with a better ux the app's core philosophy felt kinda incoherent. It just wasnt delivering the impact it should. So I went back to my critical consultant gpt. The prompt for this one is designed to be ruthless, no sugarcoating. I asked it to critique my app's core idea and as expected I got another bashing for how naive some of my assumptions were. This led to a third revamp from the ground up, but I was enjoying it because I was learning so much so fast.\n\nMy whole workflow transformed. Creating specialized gpts, my ux guy, my strategy consultant, a junior coder for boilerplate, it felt less like I was a solo dev and more like a founder managing a team. It was a genuine moment of realization for me. I know it might sound obvious but I finally understood on a gut level why good founders obsess over hiring specialists. My job shifted completely. I was now spending 95% of my time on high level decisions, directing and interrogating the logic from my AI 'staff' instead of just coding.\n\nI can't overstate the accelerated learning I got from this. It's a different way of building things. Using AI not just to write code but to challenge your own thinking and expose your blind spots.\n\nthere's a lot of doom and gloom around AI right now, but my experience with this project left me with a different feeling, for me the real power here isn't about replacing people. It's about giving a single person the leverage to do things that used to require a whole team. It felt less like I was being made obsolete, and more like I was being augmented. That feels like a silver lining worth talking about.\n\nAnyway, that's my story so far. The prompts themselves are nothing revolutionary, but if you want to see them or the final app for context, just let me know in the comments.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 18:11",
    "_score": 4
  },
  {
    "title": "One-Minute Daily AI News 8/8/2025",
    "link": "https://www.reddit.com/r/ArtificialInteligence/comments/1mlghe5/oneminute_daily_ai_news_882025/",
    "content": " OpenAI beats Elon Musk’s Grok in AI chess tournament.[1] Uvalde schools to install AI gun detection system on all security cameras.[2] Black Hat: Researchers demonstrate zero-click prompt injection attacks in popular AI agents.[3] RIP, Microsoft Lens, a simple little app that’s getting replaaced by AI.[4]  Sources included at: https://bushaicave.com/2025/08/08/one-minute-daily-ai-news-8-8-2025/  &#32; submitted by &#32;  /u/Excellent-Target-847   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nWe value your privacy\n\nReddit and its partners use cookies and similar technologies to provide you with a better experience. By accepting all cookies, you agree to our use of cookies to deliver and maintain our services and site, improve the quality of Reddit, personalize Reddit content and advertising, and measure the effectiveness of advertising. By rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of our platform. For more information, please see our Cookie Notice and our Privacy Policy.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-09 04:26",
    "_score": 4
  },
  {
    "title": "[Doc] Add usage of implicit text-only mode  (#22561)",
    "link": "https://github.com/vllm-project/vllm/commit/23472ff51cdf25c2f9c9bf9afa50a8d3cc6cc1d8",
    "content": "[Doc] Add usage of implicit text-only mode  (#22561)\n\nSigned-off-by: Roger Wang &lt;hey@rogerw.me&gt;\nCo-authored-by: Flora Feng &lt;4florafeng@gmail.com&gt;",
    "full_content": "[Doc] Add usage of implicit text-only mode (#22561) · vllm-project/vllm@23472ff · GitHub\n\n+\n\nFor hybrid-only models such as Llama-4, Step3 and Mistral-3, a text-only mode can be enabled by setting all supported multimodal modalities to 0 (e.g, ` --limit-mm-per-prompt '{\"image\":0} ` ) so that their multimodal modules will not be loaded to free up more GPU memory for KV cache.",
    "source": "github.com",
    "group": "community_signals",
    "date": "2025-08-09 06:04",
    "_score": 3
  },
  {
    "title": "[BugFix] [P/D] Handle lookahead token count edge-case with Eagle Spec…",
    "link": "https://github.com/vllm-project/vllm/commit/35afe1b30b154114dc2ee8329e12f8cf3fe9f576",
    "content": "[BugFix] [P/D] Handle lookahead token count edge-case with Eagle Spec Decoding and P/D (#22317)\n\nSigned-off-by: Pradyun Ramadorai &lt;pradyunr@amazon.com&gt;\nSigned-off-by: Pradyun92 &lt;142861237+Pradyun92@users.noreply.github.com&gt;\nCo-authored-by: Pradyun Ramadorai &lt;pradyunr@amazon.com&gt;\nCo-authored-by: Nicolò Lucchesi &lt;nicolo.lucchesi@gmail.com&gt;",
    "full_content": "[BugFix] [P/D] Handle lookahead token count edge-case with Eagle Spec… · vllm-project/vllm@35afe1b\n\nWe read every piece of feedback, and take your input very seriously.\n\nInclude my email address so I can be contacted",
    "source": "github.com",
    "group": "community_signals",
    "date": "2025-08-09 00:04",
    "_score": 3
  },
  {
    "title": "Accessibility and the agentic web",
    "link": "https://tetralogical.com/blog/2025/08/08/accessibility-and-the-agentic-web/",
    "content": "Comments",
    "full_content": "Accessibility and the agentic web Posted on Friday, 8 August 2025 by Léonie Watson in Strategy, User experience\n\nImagine being in a department store that sells clothes from multiple brands and having a personal shopping assistant to help you select the clothes you want to buy. As a blind person, that's about the only way it's possible to go clothes shopping, independently at least, but few stores offer such a service, so you resort to shopping online.\n\nExcept that retail websites are rarely accessible enough to make purchasing clothes much more than an exercise in “buy six things and return five\". This isn't necessarily because the websites themselves are inaccessible, in the sense of being able to search for products, find out a little about them, add them to your basket or checkout, though that's not uncommon. It's that even the most accessible retail websites don't provide enough information to let you confidently make a choice, unless you can see the product images that is.\n\nLet's take an example from a UK high street retailer:\n\nLong sleeve Knitted jumper Crew neck It comes in black 100% Cotton.\n\nYou might think that's an adequate description, but if I were to ask you if the jumper is plain or ribbed, or if it falls to the hips or thighs in length, or if it's lightweight or chunky, could you tell me?\n\nA different UK retailer does a bit better:\n\nCrafted from soft pure merino wool, this jumper is a luxurious addition to your knitwear collection. It's crafted to a relaxed fit for endless layering options. A flattering v-neckline keeps things stylish, while cosy long sleeves with ribbed trims add a neat finish to this timeless piece.\n\nBut even so, what does relaxed mean, is it a deep V-neck, is the rib trim just on the sleeves or around the neck and/or bottom too?\n\nEnter AI\n\nArtificial Intelligence (AI) tools that can generate image descriptions are helpful in this context. My screen reader now has a feature that lets me request an image description from ChatGPT or Claude without having to switch contexts to do it.\n\nThere's no denying the fact that the people responsible for these websites should do a much better job of providing useful text descriptions, but we've been having that conversation for 30 years now, so forgive me if my patience is running a little thin!\n\nThere's also no denying the egregious theft of data and consumption of natural resources by AI platforms, and I've made my feelings on that clear elsewhere.\n\nUsing AI for image descriptions comes with a risk of hallucinations, but the consequences of falling for a hallucinated description don't put me at any more of a disadvantage than having no text descriptions at all. It's essentially a judgement call.\n\nIt's also not a great experience. It depends on the retailer putting the images where my screen reader can find them, or those images having enough of a text description to help me narrow down the choice of which images to query using AI, and on the content of the images themselves of course. Even under the best of circumstances, all of this takes time. A lot of time in fact.\n\nEnter agentic AI\n\nSo, what if there was a different way? What if I could ask a digital personal shopping assistant to help me find clothes based on my preferred parameters? What if I could ask the assistant for product descriptions, then have a conversation with it to find out more, before letting the assistant know which one I'd like to purchase?\n\nYou may not know that this is already possible. It's still nascent technology, but it exists, and it's being used today to improve the accessibility and usability of the retail shopping experience.\n\nAn example is Innosearch, an AI ecommerce platform available in the USA, Canada, and the UK. It lets you shop from 500,000 online stores including those where I found the products mentioned above. Talking of which, it turns out that the long sleeved crew-neck jumper I found is for men... a minor detail that isn't obvious (if you use a screen reader) on the retailer's own product page!\n\nIn addition to presenting all products in a consistent and more accessible way than the websites they originated from, Innosearch automatically pulls out information that I'd otherwise have to actively look for - details only available in images for example. That isn't the interesting thing though. Innosearch has an agentic AI called CoBrowse, that goes beyond the behaviour of a typical AI chatbot.\n\nIf you're not familiar with the term \"agentic\", it refers to something that has agency, meaning it's capable of acting in its own right. Agentic AI is therefore a form of AI that can take actions on your behalf, as well as answer questions like a chatbot.\n\nIn this context, it means you can ask CoBrowse to search for a particular product. When I say \"ask\", I mean just that - you can enable your microphone and converse with it if typing isn't convenient. From there, it's a lot like having an actual personal shopping assistant in a bricks and mortar store. ",
    "source": "tetralogical.com",
    "group": "community_signals",
    "date": "2025-08-09 16:44",
    "_score": 3
  },
  {
    "title": "IBM and Moderna simulated the longest mRNA pattern without AI",
    "link": "https://www.livescience.com/technology/computing/ibm-and-moderna-have-simulated-the-longest-mrna-pattern-without-ai-they-used-a-quantum-computer-instead",
    "content": "Article URL: https://www.livescience.com/technology/computing/ibm-and-moderna-have-simulated-the-longest-mrna-pattern-without-ai-they-used-a-quantum-computer-instead\nComments URL: https://news.ycombinator.com/item?id=44848717\nPoints: 1\n# Comments: 0",
    "full_content": "IBM and Moderna have simulated the longest mRNA pattern without AI — they used a quantum computer instead\n\nResearchers at IBM and Moderna have successfully used a quantum simulation algorithm to predict the complex secondary protein structure of a 60-nucleotide-long mRNA sequence, the longest ever simulated on a quantum computer .\n\nMessenger ribonucleic acid (mRNA) is a molecule that carries genetic information from DNA to ribosomes. It directs protein synthesis in cells and is used to create effective vaccines capable of instigating specific immune responses.\n\nIt’s widely believed that all the information required for a protein to adopt the correct three-dimensional conformation is provided by its amino acid sequence or \"folding.\"\n\nAlthough it’s made up of only a single strand of amino acids, mRNA has a secondary protein structure consisting of a series of folds that provide a given molecule’s specific 3D shape. The number of possible folding permutations increases exponentially with each added nucleotide. This makes the challenge of predicting what shape a mRNA molecule will take intractable at higher scales.\n\nThe IBM and Moderna experiment, outlined in a study first published for the 2024 IEEE International Conference on Quantum Computing and Engineering, demonstrated how quantum computing can be used to augment the traditional methods for making such predictions. Traditionally, these predictions typically relied on binary, classical computers and artificial intelligence (AI) models such as Google DeepMind’s AlphaFold .\n\nRelated: DeepMind's AI program AlphaFold3 can predict the structure of every protein in the universe — and show how they function\n\nAccording to a new study published May 9 on the preprint arXiv database, algorithms capable of running on these classical architectures can process mRNA sequences with \"hundreds or thousands of nucleotides,\" but only by excluding higher complexity features such as \"pseudoknots.\"\n\nSign up for the Live Science daily newsletter now Get the world’s most fascinating discoveries delivered straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nPseudoknots are complicated twists and shapes in a molecule’s secondary structure that are capable of engaging in more complex internal interactions than ordinary folds. Through their exclusion, the potential accuracy of any protein-folding prediction model is fundamentally limited.\n\nUnderstanding and predicting even the smallest details of a mRNA molecule’s protein folds is intrinsic to developing stronger predictions and, as a result, more effective mRNA-based vaccines .\n\nScientists hope to overcome the limitations inherent in the most powerful supercomputers and AI models by augmenting experiments with quantum technology. The researchers conducted multiple experiments using quantum simulation algorithms that relied on qubits — the quantum equivalent of a computer bit — to model molecules.\n\nInitially using only 80 qubits (out of a possible 156) on the R2 Heron quantum processing unit (QPU),, the team employed a conditional value-at-risk-based variational quantum algorithm (CVaR-based VQA) — a quantum optimization algorithm modeled after certain techniques used to analyze complex interactions such as collision avoidance and financial risk assessment techniques — to predict the secondary protein structure of a 60-nucleotide-long mRNA sequence.\n\nThe previous best for a quantum-based simulation model, according to the study , was a 42-nucleotide sequence. The researchers also scaled the experiment by applying recent error-correction techniques to deal with the noise generated by quantum functions .\n\nIn the new preprint study, the team provisionally demonstrated the experimental paradigm’s effectiveness in running simulated instances with up to 156 qubits for mRNA sequences of up to 60 nucleotides. They also conducted preliminary research demonstrating the potential to employ up to 354 qubits for the same algorithms in noiseless settings.\n\nOstensibly, increasing the number of qubits used to run the algorithm, while scaling the algorithms for additional subroutines, should lead to more accurate simulations and the ability to predict longer sequences, they said.\n\nThey noted, however, that “these methods necessitate the development of advanced techniques for embedding these problem-specific circuits into the existing quantum hardware,” — indicating that better algorithms and processing architectures will be needed to advance the research.",
    "source": "livescience.com",
    "group": "community_signals",
    "date": "2025-08-09 18:12",
    "_score": 2
  },
  {
    "title": "AI's \"Just Ship it.\" problem",
    "link": "https://www.leahtharin.com/p/ais-just-ship-it-problem",
    "content": "Article URL: https://www.leahtharin.com/p/ais-just-ship-it-problem\nComments URL: https://news.ycombinator.com/item?id=44848484\nPoints: 1\n# Comments: 0",
    "full_content": "AI's \"Just Ship it.\" problem\n\nDo you, a B2B SaaS product, want to sponsor this spot so it’s free forever and get your message out? Drop us a line for the Mediakit: sponsoring@productea.io\n\n\"Let's fire everyone who's junior and midlevel. We have AI now!\", is AI really boosting everyone's productivity to the wazoo, or is it shifting bottlenecks?\n\nEveryone who actually does the work knows that while AI gains are impressive across the board, they are not as crazy as we might think.\n\nThe other day, I watched a video titled “Does AI actually boost developer Productivity?” by Stanford, conducted on 100,000 developers.\n\nIt attempts to ground the real numbers in terms of productivity gains, specifically for developers. There are a lot of parallels, though, to other jobs as well, including product and growth teams.\n\nI joined Smallpdf back in 2019 when we were some of the best at throwing things at the wall to see what stuck. We were about 20 highly talented people with a strong engineering and shipping focus.\n\nIt made the company successful and grow at a breakneck speed. I also remember what my CEO, Dennis Just, told me on the first day of work:\n\n“We’re really good at throwing spaghetti at the wall and seeing what sticks, but we need to grow out from that; it’s become unmanageable, and growth will eventually stall.”\n\nHow right he was.\n\nLet’s look first at some brief numbers from the study and then at the parallels to what it means for companies and their products, and why the term “shipping” is misunderstood.\n\n\"As codebase size increases, productivity gain from AI decreases\"\n\nThis is also true for other jobs. The meaningless, \"simple\" busywork can be massively improved with AI but complex work still suffers from being something an AI cannot comprehend.\n\nWe're looking at a range from simple, 60% to complex, 5 % more efficiency:\n\nMost codebases are in terms of LoC (Lines of Code) somewhere in the middle, meaning improvements are along the 20% margin.\n\nA different view on the same hypothesis: for simple complexity tasks AI is very meaningful, on high complexity less so\n\nThese numbers are in a field (engineering) that is “relatively” measurable. The worry I have is the usual lemmingesque behaviour of CEO’s not understanding any of it and translating numbers they hear on podcasts from “massive increases” to “I think we can fire half of our staff”.\n\n“Using AI in software engineering increases rework”\n\nYou're delivering more and shipping more. Not all of it is useful; not only is that stuff also requiring rework. It usually requires rework that AI can’t solve by itself.\n\nThis is especially the case when we talk about complex system integration, where you have to make various systems work with each other.\n\nThe spaghetti effect and “shipping”\n\nThe relative position you inhabit in a market is, to a large degree, influenced by your competition’s capabilities. And in 2019, Smallpdf was well ahead of its competition. We dominated our distribution (SEO) and were shipping things multiple times a day.\n\nThrowing spaghetti on the wall to see what stuck and reiterating on it led us to success with impressive numbers, hypergrowth at a scale that most companies and funds dream of.\n\nAt its height, we had 50,000,000 monthly active users using our services to play around (worked on by about 10 teams, product & growth). This traffic was divided into the few dozen tools we had, which was impressive by any standard.\n\nAnd while this was by far the biggest data stack I’ve worked on at that time, I still ran into problems at scale:\n\nThe bottleneck of the team was waiting for statistical significance for most of our experiments, despite all the traffic we had.\n\nShipping a lot of small crap without validating it is the same as polishing a turd. The product constantly changes, but not necessarily for the better.\n\nThe company was struggling to come up with big, good swings that were worth building. Should we add editing of a PDF into the product, or make all tools collaborative? Should we ship less so there’s less disruption to the service? Should we rework the legacy code that has not been touched in 4 years for most tools?\n\nEspecially the first point was amazing to me. If we changed a copy of our main website or most prominent tools, the experiments were statistically significant within hours.\n\nA more complex down the funnel change for higher value customers? Weeks, sometimes months.\n\nUgh.\n\nIf we had access to AI improvements and faster devs, none of that would have changed. You can ship reiterations faster, but I remember specifically that my planning was constrained by how many tries I had to improve things with the team per given week.\n\nIf you haven’t yet, you will run into the same problem. Regardless of how many people you reach.\n\nThe term “shipping”\n\n“Just ship it” was the go-to phrase of a manager I used to have over 20 years ago. It annoyed me to no end. He constantly said it because he felt that everyone was slow and not keeping up with hi",
    "source": "leahtharin.com",
    "group": "community_signals",
    "date": "2025-08-09 17:43",
    "_score": 2
  },
  {
    "title": "AI Has No Butt to Clench",
    "link": "https://www.bootstoobig.com/p/ai-has-no-butt-to-clench",
    "content": "Article URL: https://www.bootstoobig.com/p/ai-has-no-butt-to-clench\nComments URL: https://news.ycombinator.com/item?id=44848444\nPoints: 2\n# Comments: 0",
    "full_content": "AI Has No Butt To Clench\n\nOn Feb 26th, Slack went down for 9 hours\n\nAs the technical lead for a major project that was going through launch week, my butt was clenched.\n\nWhile I scrambled to assemble a coordination email, the thought occurred to me: Who else cares? Who feels impacted by the delay caused by the outage? Whose butt is also clenched?\n\nAnd more cynically: who doesn’t care? Who is treating this as an impromptu vacation day?\n\nYes, that’s a terribly toxic thing to think about your teammates. But it struck me that especially during an outage, Slack sure looks a hell of a lot like ChatGPT:\n\nThe similarities aren’t just visual. In both Slack and ChatGPT, text goes in, text comes out. Sometimes work is done, sometimes someone looks something up, sometimes it’s high quality responses, sometimes it’s low quality, and sometimes you just don’t hear back.\n\nI’m not the first to notice this either. This is the product thesis behind Devin.ai’s agentic coding agent.\n\nAnd everyone seems to be building something similar: Cursor BugBot, OpenAI Codex, Google's Jules, etc. These companies won’t say the word “replacement”, but you can bet their sales team will certainly imply it.\n\nSo then what makes someone irreplaceable by AI? While I was thinking about this, my phone buzzed:\n\nMessage: Hey it’s Coworker, found your number in an old doc Message: What do you think, should we delay launch?\n\n^ This is Coworker, a coworker of mine. Coworker is a significant contributor and stakeholder for the milestones we were launching this week. Coworker is stressed that Slack is down because Coworker is partially accountable if the launch goes poorly. Coworker’s butt is definitely clenched.\n\nAI Has No Butt To Clench\n\nHere is the thesis in a nutshell:\n\nAs your leadership grows, so does your risk (accountability for bad outcomes).\n\nAs your risk grows, you will offload risk and responsibility to other budding leaders.\n\nAI cannot feel risk, so it cannot own risk. You cannot offload risk to AI.\n\nThus, to avoid being replaced by AI, you must master the thing it cannot do: owning risk.\n\nWhat is risk?\n\nRisk isn’t just the big stuff like AI turning the world into paperclips or ordering 4 thousand pounds of meat.\n\nIt’s also smaller, personal stakes. Imagine convincing friends to see a concert that turns out to be awful. You feel that pit in your gut because you made the decision.\n\nThat’s risk. You cringe at the outcome of your decisions, AI doesn’t.\n\nThink about who is being sued right now? ChatGPT? Nope. You can’t blame ChatGPT. Instead you blame:\n\nYou can’t sue ChatGPT because ChatGPT can't be accountable. ChatGPT can’t feel risk, so ChatGPT can’t own risk. ChatGPT can’t clench because ChatGPT has no butt to clench.\n\nLeadership is risk\n\nWhat is all this butt-clenching about? Good question.\n\nWhen:\n\nThe CEO demos your team’s project\n\nYou click the “launch” button\n\nOr something breaks in a big and public way\n\nYou can tell who the leaders are (who owns the most risk), by whose butt is most clenched.\n\nLeaders survive by offloading risk\n\nAs leaders become more and more senior, they will own more “landscape” (as a student put it), and therefore they will own more risk.\n\nAt some point, this becomes untenable. One person cannot be responsible for everything that goes wrong on Airbnb.com.\n\nSo how do leaders survive? Typically, they look for budding leaders in their proximity to share the risk and responsibilities with. This grows leadership in others by delegating increasingly larger accountability.\n\nSo what happens when you try to offload risk to AI? Well:\n\nAI can’t own risk\n\nAI doesn’t cringe if it gives a bad concert recommendation.\n\nAI doesn’t care if it’s building a demo for the CEO or just for you.\n\nAI doesn’t stress if the button it just clicked was a “launch” vs “test” button.\n\nAI can’t be fired or promoted. It doesn’t worry about whether its peers think it's incompetent. It doesn’t care about its reputation. It doesn’t stay up at night cringing at that dumb thing it said.\n\nAI can’t feel the repercussions for the decisions it makes.\n\nAI can’t feel risk, so AI can’t own risk.\n\nAI can’t clench its butt because AI has no butt to clench.\n\nWhen you delegate risk to AI, your risk actually amplifies. Not only is it unaccountable for the decisions it makes, but it can make a thousand decisions in the time it takes you to make one. Now you are accountable for those thousand decisions.\n\nLook at where AI is useful today:\n\nFrontend-leaning prototypes to flesh out new ideas and capture new markets\n\nSummarizing information (read-only applications)\n\nWriting tests and building redundancy in systems\n\nResearch and deep knowledge aggregation\n\nSuggesting initial ideas to get over the activation energy of starting a hard task\n\nDo you notice the common thread? None of these require any real risk. They delegate the risk to the invoker.\n\nWhere does AI fail today?\n\nReal business logic complexity failures\n\nWiping production databases\n\nHallucinating fake legal cases\n\nBui",
    "source": "bootstoobig.com",
    "group": "community_signals",
    "date": "2025-08-09 17:37",
    "_score": 2
  },
  {
    "title": "Why Your AI Never Works on the First Try",
    "link": "https://fluxus.io/article/why-your-ai-never-works-on-the-first-try",
    "content": "Article URL: https://fluxus.io/article/why-your-ai-never-works-on-the-first-try\nComments URL: https://news.ycombinator.com/item?id=44848243\nPoints: 1\n# Comments: 0",
    "full_content": "Why Your AI Never Works on the First Try\n\nIt's 3am. You're staring at ChatGPT, thinking \"just one more try.\"\n\nYou've been iterating on the same email for an hour. Each attempt gets slightly closer to sounding human. The tone improves, the awkward phrases decrease, but something's still off. You know you're close - you can feel it - but you have no idea if you're one attempt away or ten.\n\nThis is the universal experience of AI power users. That peculiar exhaustion of grinding through iterations, knowing you're making progress but unable to see the destination. Like being stuck on the same level of a video game, except the level changes slightly each time you play it.\n\nI came back to coding after 13 years away. Within weeks I was shipping real systems - the AI's power was obvious. But I noticed something strange. Everything took multiple attempts to get right. Sometimes two, sometimes twenty. Occasionally you'd get lucky on the first try, but that was rare enough to feel like winning the lottery.\n\nAt first I thought it was me. Rusty skills, bad prompting, not understanding the tools. But the pattern was everywhere. Emails, proposals, images. And here's the weird part: it only happened when I knew what good looked like. When I ventured into unfamiliar territory, the first output seemed brilliant. When I had expertise, the grinding began.\n\nFor months I just accepted this. Everyone does. Multiple revisions of a paragraph that should take two minutes to write. Endless attempts at a Midjourney prompt. By attempt five, you're using caps lock. By attempt ten, you're swearing at it like it's deliberately being obtuse. If there was a human on the other end, HR would be involved.\n\nThen one night, deep into another session, something clicked. This is software. Deterministic, mathematical software. Imagine if Excel took multiple attempts to sum a column. If Gmail needed three tries to send an email. If Spotify required a couple of refreshes before a song would play. You'd think your computer was broken.\n\nBut we've normalised this exact behaviour in AI. We've accepted \"let me try again\" as standard operating procedure.\n\nThat night, I started asking a different question. Not \"how do I reduce iterations\" but \"why do iterations exist at all?\"\n\nThe answer changed everything I understood about what we're actually building.\n\nI started documenting the pattern obsessively. Every AI interaction, I tracked the iterations. Code, emails, images - didn't matter. The variance was maddening. Sometimes two attempts, sometimes twenty. Never predictable.\n\nAt first I thought it was randomness - LLMs are probabilistic, temperatures, sampling, all that. The people quick to explain that LLMs are non-deterministic often don't realise that at temperature 0, they're effectively deterministic. Same prompt, same output. GitHub Copilot runs near zero. Claude and GPT in production often run low.\n\nYet they still require multiple attempts. The randomness wasn't the problem.\n\nI watched what actually happens when you iterate. With code, you get an error, paste it back, the AI patches that specific issue. Another error, another patch. It's not debugging - it's something else. The AI can't see why its original wouldn't work. It needs the error message to navigate from.\n\nSame with writing. You say \"too formal\" and it lurches into casual slang. \"No, back it off\" and it swings to corporate speak. You're playing pendulum, trying to dampen the swings until it accidentally lands in the middle.\n\nThe AI wasn't learning or understanding. It was doing something more primitive. Something almost... mechanical.\n\nThen I found out I wasn't alone in noticing this. Thoughtworks had just published extensive experiments on autonomous code generation. Martin Fowler's site featured the work by Birgitta Boeckeler and her team. These aren't hobbyists. They're distinguished engineers with unlimited resources. They built sophisticated multi-agent systems, reference applications, elaborate workflows. Months of systematic work.\n\nTheir conclusion? Human oversight remains essential. They called it \"playing whac-a-mole\" - every run produced different errors requiring different fixes. They documented the pattern exhaustively.\n\nThey just couldn't explain why.\n\nThe best engineering minds in the industry had mapped the same territory I was exploring. They saw the constraint but not the cause. Multiple agents didn't help. Better prompting didn't eliminate it. More sophisticated workflows just moved the problem around.\n\nThat's when I realised we were all missing something fundamental. Something about the architecture itself.\n\nFor weeks, the pattern haunted me. The AI could generate plausible solutions but couldn't see if they'd work. It needed errors to navigate from. It overcorrected, then overcorrected the overcorrection. Always walking, never seeing ahead.\n\nThen, during another late-night session watching the AI fumble through iterations, it hit me. I'd seen this before. Not in AI, but in",
    "source": "fluxus.io",
    "group": "community_signals",
    "date": "2025-08-09 17:10",
    "_score": 2
  },
  {
    "title": "Brave Search: AI Grounding API",
    "link": "https://brave.com/blog/ai-grounding/",
    "content": "Article URL: https://brave.com/blog/ai-grounding/\nComments URL: https://news.ycombinator.com/item?id=44848083\nPoints: 2\n# Comments: 0",
    "full_content": "Introducing AI Grounding with Brave Search API, providing enhanced search performance in AI applications\n\nToday, we’re launching AI Grounding with the Brave Search API—a robust, all-in-one solution for connecting an AI system’s outputs to verifiable data sources. With AI Grounding, LLM (Large Language Model) responses are anchored in high-quality, factual information from verifiable Web sources, thus reducing hallucinations and responding more appropriately to nuanced inputs.\n\nAnswers generated using AI Grounding achieve state-of-the-art (SOTA) performance on the SimpleQA benchmark, with an F1-score of 94.1%. Notably, Brave’s AI Grounding service achieves these results without being specifically optimized for the SimpleQA benchmark—the performance emerges as a natural byproduct of the system’s design.\n\nBrave’s AI Grounding endpoint achieves state-of-the-art (SOTA) performance on SimpleQA\n\nBrave’s AI Grounding already powers the Answer with AI feature on Brave Search, which handles over 15 million queries daily, and will soon support the upcoming Research-mode features in Brave Search as well as in Brave Leo, the Brave browser’s integrated AI assistant. The Brave Search API already supplies most of the top 10 AI LLMs with real-time Web search data, and for some of them, Brave is in fact the only search engine index supporting their AI answers.\n\nGetting started with AI Grounding\n\nBeginning today, customers of the Brave Search API can access the new AI Grounding endpoint by subscribing to one of our public AI plans—Free, Base, or Pro—or by contacting us about custom plans. A high-level overview of what’s new with this release includes:\n\nAI Grounding plan (new): Answers are grounded on a single search or the more advanced Research mode (multiple searches, reasoning, planning and answer). This plan is priced at $4 per thousand web searches plus $5 per million tokens (input and output).\n\n(new): Answers are grounded on a single search or the more advanced Research mode (multiple searches, reasoning, planning and answer). This plan is priced at $4 per thousand web searches plus $5 per million tokens (input and output). Pro AI plan (update): Get access to this update via the existing openapi-compliant endpoint. The pricing of the Pro AI plan does not change.\n\nAI Grounding: Research is available today for API customers, and will soon power deep research features for end users of Brave Search and Brave Leo.\n\nTechnical notes\n\nQuality assessment at Brave relies on both AI-based evaluations and human assessments. These human evaluations are conducted by a dedicated team trained specifically to ensure consistency and coherence. The assessment process encompasses a wide range of dimensions—from navigational queries to blind evaluations of final outputs and more.\n\nOne of the AI tests we used is SimpleQA, a benchmark developed by OpenAI to assess the factual accuracy of large language models (LLMs) when answering short, fact-seeking questions. It focuses on questions with single, indisputable, and timeless answers, allowing straightforward evaluation using an LLM as a judge. The dataset includes 4,332 questions spanning diverse domains such as history, science, technology, art, and entertainment.\n\nAlthough originally intended to evaluate LLMs without access to external information, SimpleQA has also been used to test grounding capabilities—that is, how well LLMs perform when given contextual information from the Web rather than relying solely on their internal knowledge.\n\nSince the SimpleQA benchmark is frequently used as a proxy for measuring the factual accuracy of LLMs augmented with search-based information retrieval, we will examine how Brave compares to others in the industry and offer a detailed interpretation of the benchmark and some of its limitations.\n\nComparison to other API providers\n\nAn important yet often overlooked factor when evaluating performance on the SimpleQA benchmark is the number of search queries issued per question. This distinction—between single-search and multi-search strategies—can significantly influence both cost and user experience, yet it is rarely emphasized in benchmark discussions.\n\nThe choice between single-search and multi-search approaches has foundational implications:\n\nSingle-search systems issue one query to the Web, pass the results to an LLM, and generate an answer. This method is fast and cost-efficient—on average, responses from Brave Search finish streaming in under 4.5 seconds, with minimal computational overhead.\n\nissue one query to the Web, pass the results to an LLM, and generate an answer. This method is fast and cost-efficient—on average, responses from Brave Search finish streaming in under 4.5 seconds, with minimal computational overhead. Multi-search systems , by contrast, perform sequential searches. The LLM iteratively refines its understanding of the query and issues additional searches, leading to: Higher API call volumes Larger context windows to process Increase",
    "source": "brave.com",
    "group": "community_signals",
    "date": "2025-08-09 16:54",
    "_score": 2
  },
  {
    "title": "A Realistic AI Timeline",
    "link": "https://vintagedata.org/blog/posts/realistic-ai-timeline",
    "content": "Article URL: https://vintagedata.org/blog/posts/realistic-ai-timeline\nComments URL: https://news.ycombinator.com/item?id=44847951\nPoints: 5\n# Comments: 1",
    "full_content": "A Realistic AI Timeline\n\nAI timelines can grow old. Reading through the last highly publicized exercise, it's as if we were stuck in the early 2020s: ever larger models, unlocking ever more impressive emerging capacities. GPT-3 steampunk.\n\nWe are already in a different cycle: pretraining as we know it is ending. Reasoning, reinforcement learning, mid and post-training are drawing most of the focus. The first productivized agents will nearly certainly be relatively small models.\n\nIn short, it's time to reopen the timelines, introduce a bit more entropy in the possible paths to come. We take as a starting point a very different set of premises that is best summarized in one graph:\n\nRoughly: generalist scaling does not work or, at least, not well enough to make meaningul sense for material deployment. Instead, most development, including agentification, happens in the smaller size range with specialized, opinionated training. Any actual \"general intelligence\" has to take an entirely different direction — one that is almost discouraged by formal evaluation. Simply put: the first AGI will be bad, also amazing in other ways but still bad.\n\nThis narrative is not realistic in the sense it will happen. It is realistic insofar it is grounded in the current state of AI research around mi-2025. It is further anchored in my practical experience on the field: I pretrained language models end to end, I know what they can achive but also where they fall short and what needs solving before we move on to more speculative tech.\n\nFinally, this is a fast timeline. Simply out of formal convenience, as decades of lag do not really make up for good writing. You can easily multiply the years by a factor of two or even ten and still be in the right. Just fasten your seat belt and here we go.\n\n2026: Clearing for take-off\n\nSince it's a short timeline, let's start with a bang: in 2026, generative AI finally happens. The combined revenue of the entire sector that was stuck under 10 billions or so suddenly explode by one or two order of magnitude and finally catch up with old AI.\n\nThe main block is lifted: major use cases in services or industries have a low tolerance for errors, in the order of 0-2%. As high as 20% for credit scoring but almost anything from supplied chains check to insurance will be one or two magnitude order smaller. Many existing ISO standards even mandates a zero error rate. Despite their limitations, rule-based approach can get pretty good at this — typically, elevators are fundamentally a rule-based system and one of the safest engine in the world.\n\nWhile safe and accurate deployment can be achieved with a human in the loop, this process only makes economic sense once an AI system is already resilient enough. You don't want to manually correct 20% of the time. In short, the dynamic that matter it is not a timeline of capacities but a timeline of accuracies. As LDJ showed a few weeks ago this instantly shift estimates by quite a few years.\n\nTo enforce our short timeline, let's include a few more disruptive factors. In the current state of technological development, the main one could be the fast democratization of reinforcement learning and synthetic reasoning strategies, becoming so widespread to the point of replacing most machine learning routines. We already know by now that the classic inference time O1/R1 approach can be highly effective on a variety of formal tasks. Roughly, instead of predicting the next token we suddenly start rewarding the expected features of a \"good answer\" and let the model yaps its way around it. In practice, this means not only even small model getting performant in math, but also largely solving internally the lingering issue of structured generation: all specialized LLMs are now API ready, immediately pluggable on existing infrastructures.\n\nWe might already have all the fundamental bricks to get there what is missing is a large scale diversification. The unfortunate reality is that, for now, RL is not transferring well across domain and most domains are not covered by existing reward functions, mostly designed around code and math. Actually, most domains are not even \"verifiable\" in the sense there won't be a universal unique answer but a gradient of better one. So it takes a village to build the vertical RL ecosystem: operationalized rewards, rubric engineering, classifiers, llm-as-a-judge.\n\nAt this point, even tiny reasoning models could suddenly become good in a large variety of fields. At Pleias, we're doing a series of early experimentations on semantic data in regulated industries (especially banking and telecommunication). So far a gpt-2 sized model can be leveraged to a get a deeper and more accurate of industry standards than frontier models. Logical reasoning performs well even in the gpt-2 size. Acculturation to sector-specific norms and knowledge does the rest.\n\nStill something is missing to really move toward nearly instantaneous adoption. if you see generative models ",
    "source": "vintagedata.org",
    "group": "community_signals",
    "date": "2025-08-09 16:41",
    "_score": 2
  },
  {
    "title": "Why AI's Tom Cruise problem means it is 'doomed to fail' (2024)",
    "link": "https://www.theguardian.com/technology/article/2024/aug/06/ai-llms",
    "content": "Article URL: https://www.theguardian.com/technology/article/2024/aug/06/ai-llms\nComments URL: https://news.ycombinator.com/item?id=44847793\nPoints: 2\n# Comments: 0",
    "full_content": "Why AI’s Tom Cruise problem means it is ‘doomed to fail’\n\nIn 2021, linguist Emily Bender and computer scientist Timnit Gebru published a paper that described the then-nascent field of language models as one of “stochastic parrots”. A language model, they wrote, “is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning.”\n\nThe phrase stuck. AI can still get better, even if it is a stochastic parrot, because the more training data it has, the better it will seem. But does something like ChatGPT actually display anything like intelligence, reasoning, or thought? Or is it simply, at ever-increasing scales, “haphazardly stitching together sequences of linguistic forms”?\n\nInside the AI world, the criticism is typically dismissed with a hand wave. When I spoke to Sam Altman last year, he sounded almost surprised to be hearing such an outdated critique. “Is that still a widely held view? I mean is that considered – are there still a lot of serious people who think that way,” he asked.\n\nView image in fullscreen OpenAI CEO Sam Altman. Photograph: Jason Redmond/AFP/Getty Images\n\n“My perception is, after GPT-4, people mostly stopped saying that and started saying ‘OK, it works, but it’s too dangerous.’” GPT-4, he said, was reasoning, “to a small extent”.\n\nSometimes, the debate feels semantic. What does it matter if the AI system is reasoning or simply parroting if it can tackle problems previously beyond the ken of computing? Sure, if you’re trying to create an autonomous moral agent, a general intelligence capable of succeeding humanity as the protagonist of the universe, you might want it to be able to think. But if you’re just making a useful tool – even if it’s useful enough to be a new general purpose technology – does the distinction matter?\n\nTokens not facts\n\nTurns out, yes. As Lukas Berglund, et al wrote last year:\n\nIf a human learns the fact, “Valentina Tereshkova was the first woman to travel to space”, they can also correctly answer, “Who was the first woman to travel to space?” This is such a basic form of generalization that it seems trivial. Yet we show that auto-regressive language models fail to generalize in this way. This is an instance of an ordering effect we call the Reversal Curse.\n\nThe researchers “taught” a bunch of fake facts to large language models, and found time and again that they simply couldn’t do the base work of inferring the reverse. But the problem doesn’t simply exist in toy models or artificial situations:\n\nWe test GPT-4 on pairs of questions like, “Who is Tom Cruise’s mother?” and, “Who is Mary Lee Pfeiffer’s son?” for 1,000 different celebrities and their actual parents. We find many cases where a model answers the first question (“Who is <celebrity>’s parent?”) correctly, but not the second. We hypothesize this is because the pretraining data includes fewer examples of the ordering where the parent precedes the celebrity (eg “Mary Lee Pfeiffer’s son is Tom Cruise”).\n\nOne way to explain this is to realise that LLMs don’t learn about relationships between facts, but between tokens, the linguistic forms that Bender described. The tokens “Tom Cruise’s mother” are linked to the tokens “Mary Lee Pfeiffer”, but the reverse is not necessarily true. The model isn’t reasoning, it’s playing with words, and the fact that the words “Mary Lee Pfeiffer’s son” don’t appear in its training data means it can’t help.\n\nBut another way to explain it is to realise that, well, humans are also asymmetric in this way. Our reasoning is symmetric: if we know two people are mother and son, we can discuss that relationship in both directions. But our recall isn’t: it is much easier to remember fun facts about celebrities than it is to be prompted, context free, with barely recognisable gobbets of information and asked to place exactly why you know them.\n\nAt the extreme, this is obvious: compare being asked to list all 50 US states with being shown a list of 50 state names and being asked to name the country they comprise. As a question of reasoning, the facts are symmetric; as a task of recall, they very much are not.\n\nBut doctor, this man is my son\n\nskip past newsletter promotion Sign up to TechScape Free weekly newsletter A weekly dive in to how technology is shaping our lives Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy . We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion\n\nView image in fullscreen A cabbage. Not pictured: a man, a goat or their boat. Photograph: Chokchai Silarug/Getty Images\n\nThis is",
    "source": "theguardian.com",
    "group": "community_signals",
    "date": "2025-08-09 16:24",
    "_score": 2
  },
  {
    "title": "End-User Programmable AI",
    "link": "https://queue.acm.org/detail.cfm?id=3746223",
    "content": "Article URL: https://queue.acm.org/detail.cfm?id=3746223\nComments URL: https://news.ycombinator.com/item?id=44847334\nPoints: 18\n# Comments: 1",
    "full_content": "Unleashing the Power of End-User Programmable AI\n\nJuly 7, 2025\n\nVolume 23, issue 3\n\nPDF\n\nUnleashing the Power\n\nof End-User Programmable AI\n\nCreating an AI-first program Synthesis framework\n\nErik Meijer\n\nAs a demonstration of what can be accomplished with contemporary LLMs (large language models), this paper outlines the high-level design of an AI-first, program-synthesis framework built around a new programming language, Universalis, designed for knowledge workers to read, optimized for our neural computer (Automind; queue.acm.org) to execute, and ready to be analyzed and manipulated by an accompanying set of tools. We call the language Universalis in honor of Gottfried Wilhelm Leibniz, a 17th century German polymath who had an amazing vision long before our time. Leibniz's centuries-old program of a universal science (scientia universalis; wikipedia.org) for coordinating all human knowledge into a systematic whole comprises two parts: (1) a universal notation (characteristica universalis; oxfordreference.com) by use of which any item of information whatsoever can be recorded naturally and systematically, and (2) a means of manipulating the knowledge thus recorded in a computational fashion, to reveal its logical interrelations and consequences (calculus ratiocinator; wikipedia.org). Exactly what current day LLMs provide!\n\nThis may sound a bit abstract and academic, but it is extremely concrete and practical. Here is an example of a question that can be asked in natural language (the what):\n\nQuestion\n\nAlice bought a kilo of apples for $[@B]. She sold them for $[@S]. How much percent %[@P] profit or loss did Alice make?\n\nThis is the generated Universalis implementation (the how):\n\nAnswer\n\nThe apples cost $[@B], and the selling price was $[@S], so Alice made a profit of $[@D is (@S-@B)].The profit percentage is therefore [@P is (@D/@B)*100)]%.\n\nWhile this may seem like a whimsical example, it is not intrinsically easier or harder for an AI model compared to solving a real-world problem from a human perspective. The model processes both simple and complex problems using the same underlying mechanism. To lessen the cognitive load for the human reader, however, we will stick to simple targeted examples in this article.\n\nTo actually run a Universalis program, users need to be able to enter initial values for variables and view the values of computed variables. Depending on the environment in which Universalis code runs, the user interface can vary. Assuming a spreadsheet-style \"live programming\" environment where you can toggle between showing formulas and showing values, setting @B=10 and @S=17 , @P=70 and the code renders as follows:\n\nQuestion\n\nAlice bought a kilo of apples for $[10]. She sold them for $[17]. How much percent %[70] profit or loss did Alice make?\n\nAnswer\n\nThe apples cost $[10], and the selling price was $[17], so Alice made a profit of $[7 is (17-10)].The profit percentage is therefore [70 is (7/10)*100)]%.\n\nUnlike traditional programming languages, which prioritize syntax and structure optimized for writing by professional developers, Universalis is designed with the philosophy that code should be read by domain experts and written by machines.\n\nThis design tradeoff means that Universalis scripts are structured in a way that closely resembles natural language, making them intuitive and accessible even to those without formal programming training. A simple language reduces the complexity that an AI model needs to handle, making it easier for the model to generate accurate and error-free code. This shift in focus from writing to reading represents a significant departure from conventional programming paradigms, paving the way for a more inclusive and user-friendly approach to harnessing the power of AI.\n\nThink of Universalis clauses as some kind of literate Excel spreadsheet formulas such as [@D is (@S-@B)] over named tables, or relations, enclosed in hedges surrounded by natural language explanation, where cells in the table correspond to variables such as @B , @S , and @D (figure 1).\n\nThe main goal in designing Universalis was that any user who can write basic Excel formulas should be able to understand Universalis scripts. Technically, Universalis is based on Prolog, and code fragments inside [...] hedges are logic predicates.\n\nYou can also optionally specify, or let Automind generate, pre-conditions that are checked before the code is run by Automind:\n\n[@B>0] The buying price [@B] per kilo of apples must be greater than [0], implying that Alice paid a positive amount for the apples.\n\n[@S>=0] The selling price [@S] per kilo of apples must be non-negative, implying that Alice did not give away the apples for free and there was no scenario where she had to pay to dispose of them.\n\nand post-conditions that are checked after Automind has run the code:\n\nIf [@S >= @B], then the profit percentage [@P] should be non-negative, indicating a profit or break-even situation.\n\nIf [@S < @B], then the profit ",
    "source": "queue.acm.org",
    "group": "community_signals",
    "date": "2025-08-09 15:36",
    "_score": 2
  },
  {
    "title": "Show HN: Sparc3D AI – High‑Res 3D Generation Tool",
    "link": "https://sparc3dai.com",
    "content": "Article URL: https://sparc3dai.com\nComments URL: https://news.ycombinator.com/item?id=44847038\nPoints: 2\n# Comments: 0",
    "full_content": "Create High-Resolution 3D Shapes with Sparc3D\n\n1\n\nWhat is Sparc3D and how does it work?\n\nSparc3D is an advanced AI system that creates 3D models from text descriptions. It uses deep learning to understand the relationship between 3D forms and the text used to describe them, generating original models based on your prompts.",
    "source": "sparc3dai.com",
    "group": "community_signals",
    "date": "2025-08-09 15:01",
    "_score": 2
  },
  {
    "title": "[D] - What AI Engineers do in top companies?",
    "link": "https://www.reddit.com/r/MachineLearning/comments/1ml76ip/d_what_ai_engineers_do_in_top_companies/",
    "content": "Joined a company few days back for AI role. Here there is no work related to AI, it's completely software engineering with monitoring work.  When I read about AI engineers getting huge amount of salary, companies try to poach them by giving them millions of dollars I get curious to know what they do differently. Feel free to answer.  &#32; submitted by &#32;  /u/_crazy_muffin_   [link] &#32; [comments]",
    "full_content": "The heart of the internet\n\nJoined a company few days back for AI role. Here there is no work related to AI, it's completely software engineering with monitoring work.\n\nWhen I read about AI engineers getting huge amount of salary, companies try to poach them by giving them millions of dollars I get curious to know what they do differently.\n\nFeel free to answer.",
    "source": "reddit.com",
    "group": "community_signals",
    "date": "2025-08-08 21:10",
    "_score": 2
  }
]